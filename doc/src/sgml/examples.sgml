<!-- doc/src/sgml/examples.sgml -->

<part id="examples">
 <title>Examples</title>

 <partintro>
  <para>
   Various examples
  </para>

 </partintro>

 <chapter id="example-configs">
  <title>Configuration Examples</title>

  <sect1 id="example-basic">
   <title>Basic Configuration Example</title>

   <sect2 id="example-configs-begin">
    <title>Let's Begin!</title>
    <para>
     First, we must learn how to install and configure <productname>Pgpool-II</productname> and database nodes before using replication.
    </para>

    <sect3 id="example-configs-begin-installing">
     <title>Installing <productname>Pgpool-II</productname></title>
     <para>
      Installing <productname>Pgpool-II</productname> is very easy.
      In the directory which you have extracted the source tar ball,
      execute the following commands.
      <programlisting>
       $ ./configure
       $ make
       $ make install
      </programlisting>
      <command>configure</command> script collects your system information
      and use it for the compilation procedure. You can pass command
      line arguments to <command>configure</command> script to change the default behavior,
      such as the installation directory. <productname>Pgpool-II</productname>
      will be installed to <literal>/usr/local</literal> directory by default.
     </para>
     <para>
      <command>make</command> command compiles the source code, and
      <command>make install</command> will install the executables.
      You must have write permission on the installation directory.
      In this tutorial, we will install <productname>Pgpool-II
      </productname> in the default <literal>/usr/local</literal> directory.
     </para>
     <note>
      <para>
       <productname>Pgpool-II</productname> requires <literal>libpq</literal>
       library in <productname>PostgreSQL</productname> 7.4 or later (version 3 protocol).
      </para>
     </note>
     <para>
      If the <command>configure</command> script displays the following error message, the
      <literal>libpq</literal> library may not be installed, or it is not of version 3
      <programlisting>
       configure: error: libpq is not installed or libpq is old
      </programlisting>
      If the library is version 3, but the above message is still displayed, your
      <literal>libpq</literal> library is probably not recognized by the <command>
       configure</command> script.
      The <command>configure</command> script searches for <literal>libpq</literal>
      library under <literal>/usr/local/pgsql</literal>. If you have installed the
      <productname>PostgreSQL</productname> in a directory other than <literal>/usr/local/pgsql</literal>, use
      <literal>--with-pgsql</literal>, or <literal>--with-pgsql-includedir</literal>
      and <literal>--with-pgsql-libdir</literal> command line options when you
      execute <command>configure</command>.
     </para>
    </sect3>

    <sect3 id="example-configs-begin-config-files">
     <title>Configuration Files</title>
     <para>
      <productname>Pgpool-II</productname> configuration parameters are saved in the
      <literal>pgpool.conf</literal> file. The file is in <literal>"parameter = value"
      </literal> per line format. When you install <productname>Pgpool-II</productname>,
      <literal>pgpool.conf.sample</literal> is automatically created.
      We recommend copying and renaming it to <literal>pgpool.conf</literal>, and edit
      it as you like.
      <programlisting>
       $ cp /usr/local/etc/pgpool.conf.sample /usr/local/etc/pgpool.conf
      </programlisting>
      <productname>Pgpool-II</productname> only accepts connections from the localhost
      using port 9999. If you wish to receive connections from other hosts,
      set <xref linkend="guc-listen-addresses"> to <literal>'*'</literal>.
       <programlisting>
	listen_addresses = 'localhost'
	port = 9999
       </programlisting>
       We will use the default parameters in this tutorial.
     </para>
    </sect3>

    <sect3 id="example-configs-begin-config-pcp">
     <title>Configuring <acronym>PCP</acronym> Commands</title>
     <para>
      <productname>Pgpool-II</productname> has an interface for administrative
      purpose to retrieve information on database nodes, shutdown
      <productname>Pgpool-II</productname>, etc. via network. To use
      <acronym>PCP</acronym> commands, user authentication is required.
      This authentication is different from <productname>PostgreSQL</productname>'s user authentication.
      A user name and password need to be defined in the <literal>pcp.conf</literal>
      file. In the file, a user name and password are listed as a pair on each line,
      and they are separated by a colon (:). Passwords are encrypted in
      <literal>md5</literal> hash format.

      <programlisting>
       postgres:e8a48653851e28c69d0506508fb27fc5
      </programlisting>

      When you install <productname>Pgpool-II</productname>, <literal>pcp.conf.sample
      </literal> is automatically created. We recommend copying and renaming it
      to <literal>pcp.conf</literal>, and edit it.
      <programlisting>
       $ cp /usr/local/etc/pcp.conf.sample /usr/local/etc/pcp.conf
      </programlisting>
      To encrypt your password into md5 hash format, use the <command>pg_md5</command>
      command, which is installed as one of <productname>Pgpool-II</productname>'s
      executables. <command>pg_md5</command> takes text as a command line argument,
      and displays its md5-hashed text.
      For example, give <literal>"postgres"</literal> as the command line argument,
      and <command>pg_md5</command> displays md5-hashed text on its standard output.
      <programlisting>
       $ /usr/bin/pg_md5 postgres
       e8a48653851e28c69d0506508fb27fc5
      </programlisting>
      PCP commands are executed via network, so the port number must be configured
      with <xref linkend="guc-pcp-port"> parameter in <literal>pgpool.conf</literal> file.
       We will use the default 9898 for <xref linkend="guc-pcp-port"> in this tutorial.
	<programlisting>
	 pcp_port = 9898
	</programlisting>
     </para>
    </sect3>


    <sect3 id="example-configs-prep-db-nodes">
     <title>Preparing Database Nodes</title>
     <para>
      Now, we need to set up backend <productname>PostgreSQL</productname> servers for <productname>Pgpool-II
      </productname>. These servers can be placed within the same host as
      <productname>Pgpool-II</productname>, or on separate machines. If you decide
      to place the servers on the same host, different port numbers must be assigned
      for each server. If the servers are placed on separate machines,
      they must be configured properly so that they can accept network
      connections from <productname>Pgpool-II</productname>.

      <programlisting>
       backend_hostname0 = 'localhost'
       backend_port0 = 5432
       backend_weight0 = 1
       backend_hostname1 = 'localhost'
       backend_port1 = 5433
       backend_weight1 = 1
       backend_hostname2 = 'localhost'
       backend_port2 = 5434
       backend_weight2 = 1
      </programlisting>

      For <xref linkend="guc-backend-hostname">, <xref linkend="guc-backend-port">,
	<xref linkend="guc-backend-weight">, set the node's hostname, port number,
	 and ratio for load balancing. At the end of each parameter string,
	 node ID must be specified by adding positive integers starting with 0 (i.e. 0, 1, 2..).
     </para>
     <note>
      <para>
       <xref linkend="guc-backend-weight"> parameters for all nodes are
	set to 1, meaning that SELECT queries are equally distributed among
	three servers.
      </para>
     </note>
    </sect3>

    <sect3 id="example-configs-start-stop-pgpool">
     <title>Starting/Stopping <productname>Pgpool-II</productname></title>
     <para>
      To fire up <productname>Pgpool-II</productname>, execute the following
      command on a terminal.

      <programlisting>
       $ pgpool
      </programlisting>

      The above command, however, prints no log messages because <productname>
       Pgpool-II</productname> detaches the terminal. If you want to show
      <productname>Pgpool-II</productname> log messages, you pass <literal>-n</literal>
      option to <command>pgpool</command> command so <productname>Pgpool-II</productname>
      is executed as non-daemon process, and the terminal will not be detached.
      <programlisting>
       $ pgpool -n &
      </programlisting>

      The log messages are printed on the terminal, so it is recommended to use the following options.
      <programlisting>
       $ pgpool -n -d > /tmp/pgpool.log 2>&1 &
      </programlisting>

      The <literal>-d</literal> option enables debug messages to be generated.
      The above command keeps appending log messages to <literal>/tmp/pgpool.log
      </literal>. If you need to rotate log files, pass the logs to a external
      command which has log rotation function.
      For example, you can use <ulink url="https://httpd.apache.org/docs/2.4/programs/rotatelogs.html">
       <command>rotatelogs</command></ulink> from Apache2:
      <programlisting>
       $ pgpool -n 2>&1 | /usr/local/apache2/bin/rotatelogs \
       -l -f /var/log/pgpool/pgpool.log.%A 86400 &
      </programlisting>

      This will generate a log file named <literal>"pgpool.log.Thursday"</literal>
      then rotate it 00:00 at midnight. Rotatelogs adds logs to a file if it already
      exists. To delete old log files before rotation, you could use cron:
      <programlisting>
       55 23 * * * /usr/bin/find /var/log/pgpool -type f -mtime +5 -exec /bin/rm -f '{}' \;
      </programlisting>

      Please note that rotatelogs may exist as <literal>/usr/sbin/rotatelogs2</literal>
      in some distributions. <literal>-f</literal> option generates a log file as soon as
      <command>rotatelogs</command> starts and is available in apache2 2.2.9 or greater.
      Also <ulink url="http://www.cronolog.org/">cronolog</ulink> can be used.
      <programlisting>
       $ pgpool -n 2>&1 | /usr/sbin/cronolog \
       --hardlink=/var/log/pgsql/pgpool.log \
       '/var/log/pgsql/%Y-%m-%d-pgpool.log' &
      </programlisting>

      To stop <productname>Pgpool-II</productname>  execute the following command.
      <programlisting>
       $ pgpool stop
      </programlisting>

      If any client is still connected, <productname>Pgpool-II</productname>
      waits for it to disconnect, and then terminates itself. Run the following
      command instead if you want to shutdown <productname>Pgpool-II</productname>
      forcibly.
      <programlisting>
       $ pgpool -m fast stop
      </programlisting>

     </para>
    </sect3>
   </sect2>

   <sect2 id="example-configs-replication">
    <title>Your First Replication</title>
    <para>
     Replication (see <xref linkend="runtime-config-replication-mode">) enables
      the same data to be copied to multiple database nodes.
      In this section, we'll use three database nodes, which we have already set
      up in <xref linkend="example-configs-begin">, and takes you step by step to
       create a database replication system.
       Sample data to be replicated will be generated by the
       <ulink url="https://www.postgresql.org/docs/current/static/pgbench.html">
	<command>pgbench</command></ulink> benchmark program.
    </para>

    <sect3 id="example-configs-config-replication">
     <title>Configuring Replication</title>
     <para>
      To enable the database replication function, set
      <xref linkend="guc-replication-mode"> to on in <literal>pgpool.conf</literal> file.
       <programlisting>
	replication_mode = true
       </programlisting>
       When <xref linkend="guc-replication-mode"> is on, <productname>Pgpool-II</productname>
	will send a copy of a received query to all the database nodes.
	In addition, when <xref linkend="guc-load-balance-mode"> is set to true,
	 <productname>Pgpool-II</productname> will distribute <acronym>SELECT</acronym> queries
	 among the database nodes.
	 <programlisting>
	  load_balance_mode = true
	 </programlisting>
	 In this section, we will enable both <xref linkend="guc-replication-mode">
	  and <xref linkend="guc-load-balance-mode">.
     </para>
    </sect3>

    <sect3 id="example-configs-checking-replication">
     <title>Checking Replication</title>
     <para>
      To reflect the changes in <literal>pgpool.conf</literal>,
      <productname>Pgpool-II</productname> must be restarted.
      Please refer to "Starting/Stopping <productname>Pgpool-II</productname>"
      <xref linkend="example-configs-start-stop-pgpool">.
       After configuring <literal>pgpool.conf</literal> and restarting the
       <productname>Pgpool-II</productname>, let's try the actual replication
       and see if everything is working.
       First, we need to create a database to be replicated. We will name it
       <literal>"bench_replication"</literal>. This database needs to be created
       on all the nodes. Use the
       <ulink url="https://www.postgresql.org/docs/current/static/app-createdb.html">
	<command>createdb</command></ulink> commands through
       <productname>Pgpool-II</productname>, and the database will be created
       on all the nodes.
       <programlisting>
	$ createdb -p 9999 bench_replication
       </programlisting>
       Then, we'll execute <ulink url="https://www.postgresql.org/docs/current/static/pgbench.html">
	<command>pgbench</command></ulink> with <literal>-i</literal> option.
       <literal>-i</literal> option initializes the database with pre-defined tables and data.
       <programlisting>
	$ pgbench -i -p 9999 bench_replication
       </programlisting>
       The following table is the summary of tables and data, which will be created by
       <ulink url="https://www.postgresql.org/docs/current/static/pgbench.html">
	<command>pgbench -i</command></ulink>. If, on all the nodes, the listed tables and
       data are created, replication is working correctly.
     </para>

     <table id="example-configs-checking-replication-table">
      <title>data summary</title>
      <tgroup cols="2">
       <thead>
	<row>
	 <entry>Table Name</entry>
	 <entry>Number of Rows</entry>
	</row>
       </thead>

       <tbody>
	<row>
	 <entry>pgbench_branches</entry>
	 <entry>1</entry>
	</row>

	<row>
	 <entry>pgbench_tellers</entry>
	 <entry>10</entry>
	</row>

	<row>
	 <entry>pgbench_accounts</entry>
	 <entry>100000</entry>
	</row>

	<row>
	 <entry>pgbench_history</entry>
	 <entry>0</entry>
	</row>

       </tbody>
      </tgroup>
     </table>

     <para>
      Let's use a simple shell script to check the above on all the nodes.
      The following script will display the number of rows in pgbench_branches,
      pgbench_tellers, pgbench_accounts, and pgbench_history tables on all the nodes (5432, 5433, 5434).
      <programlisting>
       $ for port in 5432 5433 5434; do
       >     echo $port
       >     for table_name in pgbench_branches pgbench_tellers pgbench_accounts pgbench_history; do
       >         echo $table_name
       >         psql -c "SELECT count(*) FROM $table_name" -p $port bench_replication
       >     done
       > done
      </programlisting>

     </para>
    </sect3>
   </sect2>

  </sect1>

  <sect1 id="example-watchdog">
   <title>Watchdog Configuration Example</title>

   <para>
    This tutorial explains the simple way to try "Watchdog".
    What you need is 2 Linux boxes on which <productname>
     Pgpool-II</productname> is installed and a <productname>PostgreSQL</productname>
    on the same machine or in the other one. It is enough
    that 1 node for backend exists.
    You can use watchdog with <productname>
     Pgpool-II</productname> in any mode: replication mode,
    master/slave mode and raw mode.
   </para>
   <para>
    This example uses use "osspc16" as an Active node and
    "osspc20" as a Standby node. "Someserver" means one of them.
   </para>

   <sect2 id="example-watchdog-configuration">
    <title>Common configurations</title>
    <para>
     Set the following parameters in both of active and standby nodes.
    </para>

    <sect3 id="example-watchdog-config-enable">
     <title>Enabling watchdog</title>
     <para>
      First of all, set <xref linkend="guc-use-watchdog"> to on.
       <programlisting>
	use_watchdog = on
	# Activates watchdog
       </programlisting>
     </para>
    </sect3>

    <sect3 id="example-watchdog-config-upstream">
     <title>Configure Up stream servers</title>
     <para>
      Specify the up stream servers (e.g. application servers).
      Leaving it blank is also fine.
      <programlisting>
       trusted_servers = ''
       # trusted server list which are used
       # to confirm network connection
       # (hostA,hostB,hostC,...)
      </programlisting>
     </para>
    </sect3>

    <sect3 id="example-watchdog-config-wd-comm">
     <title>Watchdog Communication</title>
     <para>
      Specify the TCP port number for watchdog communication.
      <programlisting>
       wd_port = 9000
       # port number for watchdog service
      </programlisting>
     </para>
    </sect3>

    <sect3 id="example-watchdog-config-wd-vip">
     <title>Virtual IP</title>
     <para>
      Specify the IP address to be used as a virtual IP address
      in the <xref linkend="guc-delegate-IP">.
       <programlisting>
	delegate_IP = '133.137.177.143'
	# delegate IP address
       </programlisting>
     </para>
     <note>
      <para>
       Make sure the IP address configured as a Virtual IP should be
       free and is not used by any other machine.
      </para>
     </note>
    </sect3>
   </sect2>

   <sect2 id="example-watchdog-configuration-each-server">
    <title>Individual Server Configurations</title>
    <para>
     Next, set the following parameters for each <productname>
      Pgpool-II</productname>.
     Specify <xref linkend="guc-other-pgpool-hostname">,
      <xref linkend="guc-other-pgpool-port"> and
       <xref linkend="guc-other-wd-port"> with the values of
	other <productname>Pgpool-II</productname> server values.
    </para>

    <sect3 id="example-watchdog-configuration-active-server">
     <title>Active (osspc16) Server configurations</title>
     <para>
      <programlisting>
       other_pgpool_hostname0 = 'osspc20'
       # Host name or IP address to connect to for other pgpool 0
       other_pgpool_port0 = 9999
       # Port number for other pgpool 0
       other_wd_port0 = 9000
       # Port number for other watchdog 0
      </programlisting>
     </para>
    </sect3>

    <sect3 id="example-watchdog-configuration-standby-server">
     <title>Standby (osspc20) Server configurations</title>
     <para>
      <programlisting>
       other_pgpool_hostname0 = 'osspc16'
       # Host name or IP address to connect to for other pgpool 0
       other_pgpool_port0 = 9999
       # Port number for other pgpool 0
       other_wd_port0 = 9000
       # Port number for other watchdog 0
      </programlisting>
     </para>
    </sect3>
   </sect2>

   <sect2 id="example-watchdog-start-server">
    <title>Starting <productname>Pgpool-II</productname></title>
    <para>
     Start <productname>Pgpool-II</productname> on each servers from
     <literal>root</literal> user with <literal>"-n"</literal> switch
     and redirect log messages into pgpool.log file.
    </para>

    <sect3 id="example-watchdog-start-active-server">
     <title>Starting pgpool in Active server (osspc16)</title>
     <para>
      First start the <productname>Pgpool-II</productname> on Active server.
      <programlisting>
       [user@osspc16]$ su -
       [root@osspc16]# {installed_dir}/bin/pgpool -n -f {installed_dir}/etc/pgpool.conf > pgpool.log 2>&1
      </programlisting>
      Log messages will show that <productname>Pgpool-II</productname>
      has the virtual IP address and starts watchdog process.
      <programlisting>
       LOG:  I am announcing my self as master/coordinator watchdog node
       LOG:  I am the cluster leader node
       DETAIL:  our declare coordinator message is accepted by all nodes
       LOG:  I am the cluster leader node. Starting escalation process
       LOG:  escalation process started with PID:59449
       <emphasis>LOG:  watchdog process is initialized
	LOG:  watchdog: escalation started
	LOG:  I am the master watchdog node</emphasis>
       DETAIL:  using the local backend node status
      </programlisting>
     </para>
    </sect3>

    <sect3 id="example-watchdog-start-standby-server">
     <title>Starting pgpool in Standby server (osspc20)</title>
     <para>
      Now start the <productname>Pgpool-II</productname> on Standby server.
      <programlisting>
       [user@osspc20]$ su -
       [root@osspc20]# {installed_dir}/bin/pgpool -n -f {installed_dir}/etc/pgpool.conf > pgpool.log 2>&1
      </programlisting>
      Log messages will show that <productname>Pgpool-II</productname>
      has joined the watchdog cluster as standby watchdog.
      <programlisting>
       LOG:  watchdog cluster configured with 1 remote nodes
       LOG:  watchdog remote node:0 on Linux_osspc16_9000:9000
       LOG:  interface monitoring is disabled in watchdog
       LOG:  IPC socket path: "/tmp/.s.PGPOOLWD_CMD.9000"
       LOG:  watchdog node state changed from [DEAD] to [LOADING]
       LOG:  new outbound connection to Linux_osspc16_9000:9000
       LOG:  watchdog node state changed from [LOADING] to [INITIALIZING]
       LOG:  watchdog node state changed from [INITIALIZING] to [STANDBY]
       <emphasis>
	LOG:  successfully joined the watchdog cluster as standby node
	DETAIL:  our join coordinator request is accepted by cluster leader node "Linux_osspc16_9000"
	LOG:  watchdog process is initialized
       </emphasis>
      </programlisting>
     </para>
    </sect3>
   </sect2>

   <sect2 id="example-watchdog-try">
    <title>Try it out</title>
    <para>
     Confirm to ping to the virtual IP address.
     <programlisting>
      [user@someserver]$ ping 133.137.177.142
      PING 133.137.177.143 (133.137.177.143) 56(84) bytes of data.
      64 bytes from 133.137.177.143: icmp_seq=1 ttl=64 time=0.328 ms
      64 bytes from 133.137.177.143: icmp_seq=2 ttl=64 time=0.264 ms
      64 bytes from 133.137.177.143: icmp_seq=3 ttl=64 time=0.412 ms
     </programlisting>
     Confirm if the Active server which started at first has the virtual IP address.
     <programlisting>
      [root@osspc16]# ifconfig
      eth0      ...

      eth0:0    inet addr:133.137.177.143 ...

      lo        ...
     </programlisting>
     Confirm if the Standby server which started not at first doesn't have the virtual IP address.
     <programlisting>
      [root@osspc20]# ifconfig
      eth0      ...

      lo        ...
     </programlisting>

     Try to connect <productname>PostgreSQL</productname> by "psql -h delegate_IP -p port".
     <programlisting>
      [user@someserver]$ psql -h 133.137.177.142 -p 9999 -l
     </programlisting>
    </para>
   </sect2>

   <sect2 id="example-watchdog-vip-switch">
    <title>Switching virtual IP</title>
    <para>
     Confirm how the Standby server works when the Active server can't provide its service.
     Stop <productname>Pgpool-II</productname> on the Active server.
     <programlisting>
      [root@osspc16]# {installed_dir}/bin/pgpool stop
     </programlisting>

     Then, the Standby server starts to use the virtual IP address. Log shows:

     <programlisting>
      <emphasis>
       LOG:  remote node "Linux_osspc16_9000" is shutting down
       LOG:  watchdog cluster has lost the coordinator node
      </emphasis>
      LOG:  watchdog node state changed from [STANDBY] to [JOINING]
      LOG:  watchdog node state changed from [JOINING] to [INITIALIZING]
      LOG:  I am the only alive node in the watchdog cluster
      HINT:  skipping stand for coordinator state
      LOG:  watchdog node state changed from [INITIALIZING] to [MASTER]
      LOG:  I am announcing my self as master/coordinator watchdog node
      LOG:  I am the cluster leader node
      DETAIL:  our declare coordinator message is accepted by all nodes
      <emphasis>
       LOG:  I am the cluster leader node. Starting escalation process
       LOG:  watchdog: escalation started
      </emphasis>
      LOG:  watchdog escalation process with pid: 59551 exit with SUCCESS.
     </programlisting>

     Confirm to ping to the virtual IP address.
     <programlisting>
      [user@someserver]$ ping 133.137.177.142
      PING 133.137.177.143 (133.137.177.143) 56(84) bytes of data.
      64 bytes from 133.137.177.143: icmp_seq=1 ttl=64 time=0.328 ms
      64 bytes from 133.137.177.143: icmp_seq=2 ttl=64 time=0.264 ms
      64 bytes from 133.137.177.143: icmp_seq=3 ttl=64 time=0.412 ms
     </programlisting>

     Confirm that the Active server doesn't use the virtual IP address any more.
     <programlisting>
      [root@osspc16]# ifconfig
      eth0      ...

      lo        ...
     </programlisting>

     Confirm that the Standby server uses the virtual IP address.
     <programlisting>
      [root@osspc20]# ifconfig
      eth0      ...

      eth0:0    inet addr:133.137.177.143 ...

      lo        ...
     </programlisting>

     Try to connect <productname>PostgreSQL</productname> by "psql -h delegate_IP -p port".
     <programlisting>
      [user@someserver]$ psql -h 133.137.177.142 -p 9999 -l
     </programlisting>

    </para>
   </sect2>

   <sect2 id="example-watchdog-more">
    <title>More</title>

    <sect3 id="example-watchdog-more-lifecheck">
     <title>Lifecheck</title>
     <para>
      There are the parameters about watchdog's monitoring.
      Specify the interval to check <xref linkend="guc-wd-interval">,
       the count to retry <xref linkend="guc-wd-life-point">,
	the query to check <xref linkend="guc-wd-lifecheck-query"> and
	 finally the type of lifecheck <xref linkend="guc-wd-lifecheck-method">.
	  <programlisting>
	   wd_lifecheck_method = 'query'
	   # Method of watchdog lifecheck ('heartbeat' or 'query' or 'external')
	   # (change requires restart)
	   wd_interval = 10
	   # lifecheck interval (sec) > 0
	   wd_life_point = 3
	   # lifecheck retry times
	   wd_lifecheck_query = 'SELECT 1'
	   # lifecheck query to pgpool from watchdog
	  </programlisting>

     </para>
    </sect3>

    <sect3 id="example-watchdog-more-vip-switching">
     <title>Switching virtual IP address</title>
     <para>
      There are the parameters for switching the virtual IP address.
      Specify switching commands <xref linkend="guc-if-up-cmd">,
       <xref linkend="guc-if-down-cmd">, the path to them
	<xref linkend="guc-if-cmd-path">, the command executed after
	 switching to send ARP request <xref linkend="guc-arping-cmd">
	  and the path to it <xref linkend="guc-arping-path">.
	   <programlisting>
	    ifconfig_path = '/sbin'
	    # ifconfig command path
	    if_up_cmd = 'ifconfig eth0:0 inet $_IP_$ netmask 255.255.255.0'
	    # startup delegate IP command
	    if_down_cmd = 'ifconfig eth0:0 down'
	    # shutdown delegate IP command

	    arping_path = '/usr/sbin'           # arping command path

	    arping_cmd = 'arping -U $_IP_$ -w 1'
	   </programlisting>
	   You can also use the custom scripts to bring up and bring down the
	   virtual IP using <xref linkend="guc-wd-escalation-command"> and
	    <xref linkend="guc-wd-de-escalation-command"> configurations.
     </para>
    </sect3>

   </sect2>
  </sect1>

  <sect1 id="example-cluster">
   <title><productname>Pgpool-II</productname> + Watchdog Setup Example</title>
   <para>
    This section shows an example of streaming replication configuration using <productname>Pgpool-II</productname>. In this example, we use 3 <productname>Pgpool-II</productname> servers to manage <productname>PostgreSQL</productname> servers to create a robust cluster system and avoid the single point of failure or split brain.
   </para>
   <para>
    <productname>PostgreSQL</productname> 11 is used in this configuration example,
    all scripts have also been tested with <productname>PostgreSQL</productname> 12.
   </para>

   <sect2 id="example-cluster-structure">
    <title>Cluster System Configuration</title>
    <para>
     We use 3 servers with CentOS 7.4. Let these servers be <literal>server1</literal>, <literal>server2</literal>, <literal>server3</literal>. We install <productname>PostgreSQL</productname> and <productname>Pgpool-II</productname> on each server.
    </para>
    <para>
     <figure>
      <title>Cluster System Configuration</title>
      <mediaobject>
       <imageobject>
	<imagedata fileref="cluster_40.gif">
       </imageobject>
      </mediaobject>
     </figure>
    </para>
    <note>
     <para>
      The roles of <literal>Active</literal>, <literal>Standy</literal>, <literal>Primary</literal>, <literal>Standby</literal> are not fixed and may be changed by further operations.
     </para>
    </note>
    <table id="example-cluster-table-ip">
     <title>Hostname and IP address</title>
     <tgroup cols="3">
      <thead>
       <row>
	<entry>Hostname</entry>
	<entry>IP Address</entry>
	<entry>Virtual IP</entry>
       </row>
      </thead>
      <tbody>
       <row>
	<entry>server1</entry>
	<entry>192.168.137.101</entry>
	<entry morerows="2">192.168.137.150</entry>
       </row>
       <row>
	<entry>server2</entry>
	<entry>192.168.137.102</entry>
       </row>
       <row>
	<entry>server3</entry>
	<entry>192.168.137.103</entry>
       </row>
      </tbody>
     </tgroup>
    </table>

    <table id="example-cluster-table-postgresql-config">
     <title>PostgreSQL version and Configuration</title>
     <tgroup cols="3">
      <thead>
       <row>
	<entry>Item</entry>
	<entry>Value</entry>
	<entry>Detail</entry>
       </row>
      </thead>
      <tbody>
       <row>
	<entry>PostgreSQL Version</entry>
	<entry>11.1</entry>
	<entry>-</entry>
       </row>
       <row>
	<entry>port</entry>
	<entry>5432</entry>
	<entry>-</entry>
       </row>
       <row>
	<entry>$PGDATA</entry>
	<entry>/var/lib/pgsql/11/data</entry>
	<entry>-</entry>
       </row>
       <row>
	<entry>Archive mode</entry>
	<entry>on</entry>
	<entry>/var/lib/pgsql/archivedir</entry>
       </row>
       <row>
	<entry>Start automatically</entry>
	<entry>Disable</entry>
	<entry>-</entry>
       </row>
      </tbody>
     </tgroup>
    </table>


    <table id="example-cluster-table-pgpool-config">
     <title>Pgpool-II version and Configuration</title>
     <tgroup cols="3">
      <thead>
       <row>
	<entry>Item</entry>
	<entry>Value</entry>
	<entry>Detail</entry>
       </row>
      </thead>
      <tbody>
       <row>
	<entry>Pgpool-II Version</entry>
	<entry>4.0.2</entry>
	<entry>-</entry>
       </row>
       <row>
	<entry morerows='3'>port</entry>
	<entry>9999</entry>
	<entry>Pgpool-II accepts connections</entry>
       </row>
       <row>
	<entry>9898</entry>
	<entry>PCP process accepts connections</entry>
       </row>
       <row>
	<entry>9000</entry>
	<entry>watchdog accepts connections</entry>
       </row>
       <row>
	<entry>9694</entry>
	<entry>UDP port for receiving Watchdog's heartbeat signal</entry>
       </row>
       <row>
	<entry>Config file</entry>
	<entry>/etc/pgpool-II/pgpool.conf</entry>
	<entry>Pgpool-II config file</entry>
       </row>
       <row>
	<entry>Pgpool-II start user</entry>
	<entry>root</entry>
	<entry>See <xref linkend="TUTORIAL-WATCHDOG-START-STOP"> to startup Pgpool-II with non-root user</entry>
       </row>
       <row>
	<entry>Running mode</entry>
	<entry>streaming replication mode</entry>
	<entry>-</entry>
       </row>
       <row>
	<entry>Watchdog</entry>
	<entry>on</entry>
	<entry>Life check method: heartbeat</entry>
       </row>
       <row>
	<entry>Start automatically</entry>
	<entry>Disable</entry>
	<entry>-</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect2>

   <sect2 id="example-cluster-requirement">
    <title>Requirements</title>
    <itemizedlist>
     <listitem>
      <para>
       We assume that all the Pgpool-II servers and the <productname>PostgreSQL</productname> servers are in the same subnet.
      </para>
     </listitem>
     <listitem>
      <para>
       To use the automated failover and online recovery of <productname>Pgpool-II</productname>, 
       the settings that allow <emphasis>passwordless</emphasis> SSH to all backend servers
       between <productname>Pgpool-II</productname> execution user (default root user)
       and <literal>postgres</literal> user and between <literal>postgres</literal> user
       and <literal>postgres</literal> user are necessary. Execute the following command on all servers
       to set up passwordless <literal>SSH</literal>. The generated key file name is <literal>id_rsa_pgpool</literal>.
      </para>
      <programlisting>
       [all servers]# cd ~/.ssh
       [all servers]# ssh-keygen -t rsa -f id_rsa_pgpool
       [all servers]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
       [all servers]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
       [all servers]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server3

       [all servers]# su - postgres
       [all servers]$ cd ~/.ssh
       [all servers]$ ssh-keygen -t rsa -f id_rsa_pgpool
       [all servers]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
       [all servers]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
       [all servers]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server3
      </programlisting>
      <para>
       After setting, use <command>ssh postgres@serverX -i ~/.ssh/id_rsa_pgpool</command> command to
       make sure that you can log in without entering a password. Edit <filename>/etc/ssh/sshd_config</filename>
       if necessary and restart sshd.
      </para>
     </listitem>
     <listitem>
      <para>
       When connect to <productname>Pgpool-II</productname> and <productname>PostgreSQL</productname> servers, the target port must be accessible by enabling firewall management softwares. Following is an example for <systemitem>CentOS/RHEL7</systemitem>.
      </para>
      <programlisting>
       [all servers]# firewall-cmd --permanent --zone=public --add-service=postgresql
       [all servers]# firewall-cmd --permanent --zone=public --add-port=9999/tcp --add-port=9898/tcp --add-port=9000/tcp  --add-port=9694/udp
       [all servers]# firewall-cmd --reload
      </programlisting>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="example-cluster-installation">
    <title>Installation</title>
    <para>
     In this example, we install <productname>Pgpool-II</productname> 4.0.2 and <productname>PostgreSQL</productname> 11.1 by using RPM packages.
    </para>

    <para>
     Install <productname>PostgreSQL</productname> by using <productname>PostgreSQL</productname> YUM repository.
    </para>
    <programlisting>
     # yum install https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm
     # yum install postgresql11-server
    </programlisting>
    <para>
     Install <productname>Pgpool-II</productname> by using Pgpool-II YUM repository.
    </para>
    <programlisting>
     # yum install http://www.pgpool.net/yum/rpms/4.0/redhat/rhel-7-x86_64/pgpool-II-release-4.0-2.noarch.rpm
     # yum install pgpool-II-pg11-*
    </programlisting>
   </sect2>

   <sect2 id="example-cluster-pre-setup">
    <title>Before Starting</title>
    <para>
     Before you start the configuration process, please check the following prerequisites.
    </para>

    <itemizedlist>
     <listitem>
      <para>
       Set up <productname>PostgreSQL</productname> streaming replication on the primary server. In this example, we use WAL archiving.
      </para>
      <para>
       First, we create the directory <filename>/var/lib/pgsql/archivedir</filename> to store <acronym>WAL</acronym> segments on all servers.
      </para>
      <programlisting>
       [all servers]# su - postgres
       [all servers]$ mkdir /var/lib/pgsql/archivedir
      </programlisting>

      <para>
       Then we edit the configuration file <filename>$PGDATA/postgresql.conf</filename> on <literal>server1</literal> (primary) as follows.
      </para>
      <programlisting>
       listen_addresses = '*'
       archive_mode = on
       archive_command = 'cp "%p" "/var/lib/pgsql/archivedir/%f"'
       max_wal_senders = 10
       max_replication_slots = 10
       wal_level = replica
      </programlisting>
      <para>
       We use the online recovery functionality of <productname>Pgpool-II</productname> to setup standby server after the primary server is started.
      </para>
     </listitem>

     <listitem>
      <para>
       Because of the security reasons, we create a user <literal>repl</literal> solely used
       for replication purpose, and a user <literal>pgpool</literal> for streaming 
       replication delay check and health check of <productname>Pgpool-II</productname>. 
      </para>

      <table id="example-cluster-user">
       <title>Users</title>
       <tgroup cols="3">
        <thead>
	 <row>
	  <entry>User Name</entry>
	  <entry>Password</entry>
	  <entry>Detail</entry>
	 </row>
        </thead>
        <tbody>
	 <row>
	  <entry>repl</entry>
	  <entry>repl</entry>
	  <entry>PostgreSQL replication user</entry>
	 </row>
	 <row>
	  <entry>pgpool</entry>
	  <entry>pgpool</entry>
	  <entry>Pgpool-II health check and replication delay check user</entry>
	 </row>
	 <row>
	  <entry>postgres</entry>
	  <entry>postgres</entry>
	  <entry>User running online recovery</entry>
	 </row>
        </tbody>
       </tgroup>
      </table>

      <programlisting>
       [server1]# psql -U postgres -p 5432
       postgres=# SET password_encryption = 'scram-sha-256';
       postgres=# CREATE ROLE pgpool WITH LOGIN;
       postgres=# CREATE ROLE repl WITH REPLICATION LOGIN;
       postgres=# \password pgpool
       postgres=# \password repl
       postgres=# \password postgres
      </programlisting>

      <note>
       <para>
	If you plan to
	use <xref linkend="guc-detach-false-primary">, role
	 "pgpool" needs to
	 be <productname>PostgreSQL</productname> super user or
	 or in "pg_monitor" group to use this feature. To let
	 "pgpool" role be in the group, you can do:
	 <programlisting>
	  GRANT pg_monitor TO pgpool;
	 </programlisting>
       </para>
      </note>

      <para>
       Assuming that all the <productname>Pgpool-II</productname> servers and the 
       <productname>PostgreSQL</productname> servers are in the network of 
       <literal>192.168.137.0/24</literal>, and edit <filename>pg_hba.conf</filename> to 
       enable <literal>scram-sha-256</literal> authentication method.
      </para>
      <programlisting>
       host    all             all             samenet                 scram-sha-256
       host    replication     all             samenet                 scram-sha-256
      </programlisting>
     </listitem>

     <listitem>
      <para>
       To allow <literal>repl</literal> user without specifying password for streaming 
       replication and online recovery, we create the <filename>.pgpass</filename> file 
       in <literal>postgres</literal> user's home directory and change the permission  to
       <literal>600</literal> on each <productname>PostgreSQL</productname> server.
      </para>
      <programlisting>
       [all servers]# su - postgres
       [all servers]$ vi /var/lib/pgsql/.pgpass
       server1:5432:replication:repl:&lt;repl user password&gt;
       server2:5432:replication:repl:&lt;repl user passowrd&gt;
       server3:5432:replication:repl:&lt;repl user passowrd&gt;
       [all servers]$ chmod 600  /var/lib/pgsql/.pgpass
      </programlisting>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="example-cluster-pgpool-config">
    <title><productname>Pgpool-II</productname> Configuration</title>
    <sect3 id="example-cluster-pgpool-config-common">
     <title>Common Settings</title>
     <para>
      Here are the common settings on <literal>server1</literal>, <literal>server2</literal> and <literal>server3</literal>.
     </para>
     <para>
      When installing <productname>Pgpool-II</productname> from RPM,  all the 
      <productname>Pgpool-II</productname> configuration files are in <filename>/etc/pgpool-II</filename>. 
      In this example, we copy the sample configuration file for streaming replication mode.
     </para>
     <programlisting>
      # cp /etc/pgpool-II/pgpool.conf.sample-stream /etc/pgpool-II/pgpool.conf
     </programlisting>
     <para>
      To allow Pgpool-II to accept all incoming connections, we set <varname>listen_addresses = '*'</varname>.
     </para>
     <programlisting>
      listen_addresses = '*'
     </programlisting>
     <para>
      Specify replication delay check user and password. In this example, we leave 
      <xref linkend="GUC-SR-CHECK-USER"> empty, and create the entry in <xref linkend="GUC-POOL-PASSWD">.  
      From <productname>Pgpool-II</productname> 4.0, if these parameters are left blank, 
      <productname>Pgpool-II</productname> will first try to get the password for that 
      specific user from <xref linkend="GUC-SR-CHECK-PASSWORD"> file before using the 
      empty password. 
     </para>
     <programlisting>
      sr_check_user = 'pgpool'
      sr_check_password = ''
     </programlisting>
     <para>
      Enable health check so that <productname>Pgpool-II</> performs failover. Also, if the network is unstable, 
      the health check fails even though the backend is running properly, failover or degenerate operation may occur. 
      In order to prevent such incorrect detection of health check, we set <varname>health_check_max_retries = 3</varname>.
      Specify <xref linkend="GUC-HEALTH-CHECK-USER"> and <xref linkend="GUC-HEALTH-CHECK-PASSWORD"> in the same way like <xref linkend="GUC-SR-CHECK-USER"> and <xref linkend="GUC-SR-CHECK-PASSWORD">.
     </para>
     <programlisting>
      health_check_period = 5
                                   # Health check period
                                   # Disabled (0) by default
      health_check_timeout = 30
                                   # Health check timeout
                                   # 0 means no timeout
      health_check_user = 'pgpool'
      health_check_password = ''

      health_check_max_retries = 3
     </programlisting>
     <para>
      Specify the <productname>PostgreSQL</productname> backend information.
      Multiple backends can be specified by adding a number at the end of the parameter name.
     </para>
     <programlisting>
      # - Backend Connection Settings -

      backend_hostname0 = 'server1'
      backend_port0 = 5432
      backend_weight0 = 1
      backend_data_directory0 = '/var/lib/pgsql/11/data'
      backend_flag0 = 'ALLOW_TO_FAILOVER'

      backend_hostname1 = 'server2'
      backend_port1 = 5432
      backend_weight1 = 1
      backend_data_directory1 = '/var/lib/pgsql/11/data'
      backend_flag1 = 'ALLOW_TO_FAILOVER'

      backend_hostname2 = 'server3'
      backend_port2 = 5432
      backend_weight2 = 1
      backend_data_directory2 = '/var/lib/pgsql/11/data'
      backend_flag2 = 'ALLOW_TO_FAILOVER'
     </programlisting>
    </sect3>

    <sect3 id="example-cluster-pgpool-config-failover">
     <title>Failover configuration</title>
     <para>
      Specify failover.sh script to be executed after failover in <varname>failover_command</varname>
      parameter. 
      If we use 3 PostgreSQL servers, we need to specify follow_master_command to run after failover on the primary node failover.
      In case of two PostgreSQL servers, follow_master_command setting is not necessary.
     </para>
     <programlisting>
      failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R'
      follow_master_command = '/etc/pgpool-II/follow_master.sh %d %h %p %D %m %H %M %P %r %R'
     </programlisting>
     <para>
      Create <filename>/etc/pgpool-II/failover.sh</filename>, and add execute permission.
     </para>
     <programlisting>
      # vi /etc/pgpool-II/failover.sh
      # vi /etc/pgpool-II/follow_master.sh
      # chmod +x /etc/pgpool-II/{failover.sh,follow_master.sh}
     </programlisting>

     <itemizedlist>
      <listitem>
       <para>
        /etc/pgpool-II/failover.sh
       </para>
       <programlisting>
#!/bin/bash
# This script is run by failover_command.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&1

# Special values:
#   %d = node id
#   %h = host name
#   %p = port number
#   %D = database cluster path
#   %m = new master node id
#   %H = hostname of the new master node
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"

PGHOME=/usr/pgsql-11

logger -i -p local1.info failover.sh: start: failed_node_id=${FAILED_NODE_ID} old_primary_node_id=${OLD_PRIMARY_NODE_ID} \
    failed_host=${FAILED_NODE_HOST} new_master_host=${NEW_MASTER_NODE_HOST}

## If there's no master node anymore, skip failover.
if [ $NEW_MASTER_NODE_ID -lt 0 ]; then
    logger -i -p local1.info failover.sh: All nodes are down. Skipping failover.
    exit 0
fi

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.error failover.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

# If standby node is down, skip failover.
if [ ${FAILED_NODE_ID} -ne ${OLD_PRIMARY_NODE_ID} ]; then
    logger -i -p local1.info failover.sh: Standby node is down. Skipping failover.
    exit 0
fi

# Promote standby node.
logger -i -p local1.info failover.sh: Primary node is down, promote standby node PostgreSQL@${NEW_MASTER_NODE_HOST}.

ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -D ${NEW_MASTER_NODE_PGDATA} -w promote


if [ $? -ne 0 ]; then
    logger -i -p local1.error failover.sh: new_master_host=${NEW_MASTER_NODE_HOST} promote failed
    exit 1
fi

logger -i -p local1.info failover.sh: end: new_master_node_id=$NEW_MASTER_NODE_ID started as the primary node
exit 0
        </programlisting>
       </listitem>
      </itemizedlist>

      <itemizedlist>
       <listitem>
        <para>
/etc/pgpool-II/follow_master.sh
        </para>
        <programlisting>
#!/bin/bash
# This script is run after failover_command to synchronize the Standby with the new Primary.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&1

# special values:  %d = node id
#                  %h = host name
#                  %p = port number
#                  %D = database cluster path
#                  %m = new master node id
#                  %H = new master node host name
#                  %M = old master node id
#                  %P = old primary node id
#                  %R = new master database cluster path
#                  %r = new master port number
#                  %% = '%' character
FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"

PGHOME=/usr/pgsql-11
ARCHIVEDIR=/var/lib/pgsql/archivedir
REPL_USER=repl
PCP_USER=pgpool
PGPOOL_PATH=/usr/bin
PCP_PORT=9898


# Recovery the slave from the new primary
logger -i -p local1.info follow_master.sh: start: synchronize the Standby node PostgreSQL@${FAILED_NODE_HOST} with the new Primary node PostgreSQL@${NEW_MASTER_NODE_HOST}

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.error follow_master.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Get PostgreSQL major version
PGVERSION=`${PGHOME}/bin/initdb -V | awk '{print $3}' | sed 's/\..*//' | sed 's/\([0-9]*\)[a-zA-Z].*/\1/'`

if [ ${PGVERSION} -ge 12 ]; then
    RECOVERYCONF=${FAILED_NODE_PGDATA}/myrecovery.conf
else
    RECOVERYCONF=${FAILED_NODE_PGDATA}/recovery.conf
fi

# Check the status of standby
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
    postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -w -D ${FAILED_NODE_PGDATA} status

## If Standby is running, run pg_basebackup.
if [ $? -eq 0 ]; then

    # Execute pg_basebackup
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "

        set -o errexit
        ${PGHOME}/bin/pg_ctl -w -m f -D ${FAILED_NODE_PGDATA} stop

        rm -rf ${FAILED_NODE_PGDATA}
        rm -rf ${ARCHIVEDIR}/*

        ${PGHOME}/bin/pg_basebackup -h ${NEW_MASTER_NODE_HOST} -U ${REPL_USER} -p ${NEW_MASTER_NODE_PORT} -D ${FAILED_NODE_PGDATA} -X stream

        if [ ${PGVERSION} -ge 12 ]; then
            sed -i -e \"\\\$ainclude_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'\" \
                   -e \"/^include_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'/d\" ${FAILED_NODE_PGDATA}/postgresql.conf
        fi
      
        cat > ${RECOVERYCONF} &lt;&lt; EOT
primary_conninfo = 'host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT} user=${REPL_USER} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
EOT

        if [ ${PGVERSION} -ge 12 ]; then
            touch ${FAILED_NODE_PGDATA}/standby.signal
        else
            echo \"standby_mode = 'on'\" &gt;&gt; ${RECOVERYCONF}
        fi
    "

    if [ $? -ne 0 ]; then
        logger -i -p local1.error follow_master.sh: end: pg_basebackup failed
        exit 1
    fi

    # start Standby node on ${FAILED_NODE_HOST}
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool $PGHOME/bin/pg_ctl -l /dev/null -w -D ${FAILED_NODE_PGDATA} start

    # If start Standby successfully, attach this node
    if [ $? -eq 0 ]; then

        # Run pcp_attact_node to attach Standby node to Pgpool-II.
        ${PGPOOL_PATH}/pcp_attach_node -w -h localhost -U $PCP_USER -p ${PCP_PORT} -n ${FAILED_NODE_ID}

        if [ $? -ne 0 ]; then
            logger -i -p local1.error follow_master.sh: end: pcp_attach_node failed
            exit 1
        fi

    # If start Standby failed, drop replication slot "${FAILED_NODE_HOST}"
    else
        logger -i -p local1.error follow_master.sh: end: follow master command failed
        exit 1
    fi

else
    logger -i -p local1.info follow_master.sh: failed_nod_id=${FAILED_NODE_ID} is not running. skipping follow master command
    exit 0
fi

logger -i -p local1.info follow_master.sh: end: follow master command complete
exit 0
      </programlisting>
      </listitem>
     </itemizedlist>

    </sect3>

    <sect3 id="example-cluster-pgpool-config-online-recovery">
     <title>Pgpool-II Online Recovery Configurations</title>
     <para>
      Next, in order to perform online recovery with <productname>Pgpool-II</productname> we specify 
      the <productname>PostgreSQL</productname> user name and online recovery command 
      <command>recovery_1st_stage</command>. 
      Because Supergroup privilege of PostgreSQL is required for online recovery, we specify postgres user to <xref linkend="GUC-RECOVERY-USER">.
       Then, we create <filename>recovery_1st_stage</filename> and <filename>pgpool_remote_start</filename> 
       in database cluster directory of <productname>PostgreSQL</productname> primary server (server1), and add execute permission.

     </para>
     <programlisting>
      recovery_user = 'postgres'
      recovery_password = ''

      recovery_1st_stage_command = 'recovery_1st_stage'
     </programlisting>
     <programlisting>
      [server1]# su - postgres
      [server1]$ vi /var/lib/pgsql/11/data/recovery_1st_stage
      [server1]$ vi /var/lib/pgsql/11/data/pgpool_remote_start
      [server1]$ chmod +x /var/lib/pgsql/11/data/{recovery_1st_stage,pgpool_remote_start}
     </programlisting>

     <itemizedlist>
      <listitem>
       <para>
	/var/lib/pgsql/11/data/recovery_1st_stage
       </para>
       <programlisting>
#!/bin/bash
# This script is executed by "recovery_1st_stage" to recovery a Standby node.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&1

PRIMARY_NODE_PGDATA="$1"
DEST_NODE_HOST="$2"
DEST_NODE_PGDATA="$3"
PRIMARY_NODE_PORT="$4"
DEST_NODE_PORT=5432

PRIMARY_NODE_HOST=$(hostname)
PGHOME=/usr/pgsql-11
ARCHIVEDIR=/var/lib/pgsql/archivedir
REPL_USER=repl

logger -i -p local1.info recovery_1st_stage: start: pg_basebackup for Standby node PostgreSQL@{$DEST_NODE_HOST}

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${DEST_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.error recovery_1st_stage: passwrodless SSH to postgres@${DEST_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Get PostgreSQL major version
PGVERSION=`${PGHOME}/bin/initdb -V | awk '{print $3}' | sed 's/\..*//' | sed 's/\([0-9]*\)[a-zA-Z].*/\1/'`
if [ $PGVERSION -ge 12 ]; then
    RECOVERYCONF=${DEST_NODE_PGDATA}/myrecovery.conf
else
    RECOVERYCONF=${DEST_NODE_PGDATA}/recovery.conf
fi

## Execute pg_basebackup to recovery Standby node
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST -i ~/.ssh/id_rsa_pgpool "

    set -o errexit

    rm -rf $DEST_NODE_PGDATA
    rm -rf $ARCHIVEDIR/*

    ${PGHOME}/bin/pg_basebackup -h ${PRIMARY_NODE_HOST} -U ${REPL_USER} -p ${PRIMARY_NODE_PORT} -D ${DEST_NODE_PGDATA} -X stream

    if [ ${PGVERSION} -ge 12 ]; then
        sed -i -e \"\\\$ainclude_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'\" \
               -e \"/^include_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'/d\" ${DEST_NODE_PGDATA}/postgresql.conf
    fi

    cat &gt; ${RECOVERYCONF} &lt;&lt; EOT
primary_conninfo = 'host=${PRIMARY_NODE_HOST} port=${PRIMARY_NODE_PORT} user=${REPL_USER} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${PRIMARY_NODE_HOST}:${ARCHIVEDIR}/%f %p'
EOT

    if [ ${PGVERSION} -ge 12 ]; then
        touch ${DEST_NODE_PGDATA}/standby.signal
    else
        echo \"standby_mode = 'on'\" &gt;&gt; ${RECOVERYCONF}
    fi

    sed -i \"s/#*port = .*/port = ${DEST_NODE_PORT}/\" ${DEST_NODE_PGDATA}/postgresql.conf
"

if [ $? -ne 0 ]; then
    logger -i -p local1.error recovery_1st_stage: end: pg_basebackup failed. online recovery failed
    exit 1
fi

logger -i -p local1.info recovery_1st_stage: end: recovery_1st_stage complete
exit 0
       </programlisting>
      </listitem>

      <listitem>
       <para>
	/var/lib/pgsql/11/data/pgpool_remote_start
       </para>
       <programlisting>
#!/bin/bash
# This script is run after recovery_1st_stage to start Standby node.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&1

PGHOME=/usr/pgsql-11
DEST_NODE_HOST="$1"
DEST_NODE_PGDATA="$2"


logger -i -p local1.info pgpool_remote_start: start: remote start Standby node PostgreSQL@$DEST_NODE_HOST

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${DEST_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.error pgpool_remote_start: passwrodless SSH to postgres@${DEST_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Start Standby node
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST -i ~/.ssh/id_rsa_pgpool "
    $PGHOME/bin/pg_ctl -l /dev/null -w -D $DEST_NODE_PGDATA start
"

if [ $? -ne 0 ]; then
    logger -i -p local1.error pgpool_remote_start: PostgreSQL@$DEST_NODE_HOST start failed.
    exit 1
fi

logger -i -p local1.info pgpool_remote_start: end: PostgreSQL@$DEST_NODE_HOST started successfully.
exit 0
       </programlisting>
      </listitem>
     </itemizedlist>

     <para>
      In order to use the online recovery functionality, the functions of 
      <function>pgpool_recovery</function>, <function>pgpool_remote_start</function>, 
      <function>pgpool_switch_xlog</function> are required, so we need install 
      <function>pgpool_recovery</function> on template1 of <productname>PostgreSQL</productname> server 
      <literal>server1</literal>.
     </para>
     <programlisting>
      [server1]# su - postgres
      [server1]$ psql template1 -c "CREATE EXTENSION pgpool_recovery"
     </programlisting>
    </sect3>

    <sect3 id="example-cluster-pgpool-config-auth">
     <title>Client Authentication Configuration</title>
     <para>
      Because in the section <link linkend="EXAMPLE-CLUSTER-PRE-SETUP">Before Starting</link>, 
      we already set <productname>PostgreSQL</productname> authentication method to 
      <acronym>scram-sha-256</acronym>, it is necessary to set a client authentication by 
      <productname>Pgpool-II</productname> to connect to backend nodes. 
      Please note that only AES encrypted password or clear text password can be specified
      in <xref linkend="guc-health-check-password">, <xref linkend="guc-sr-check-password">, 
	<xref linkend="guc-wd-lifecheck-password">, <xref linkend="guc-recovery-password"> in <filename>pgpool.conf</filename>.
	  When installing with RPM, 
	  the <productname>Pgpool-II</productname> configuration file <filename>pool_hba.conf</filename> 
	  is in <filename>/etc/pgpool-II</filename>. 
	  By default, pool_hba authentication is disabled, and set <varname>enable_pool_hba = on</varname> 
	  to enable it.
     </para>
     <programlisting>
      enable_pool_hba = on
     </programlisting>
     <para>
      The format of <filename>pool_hba.conf</filename> file follows very closely PostgreSQL's 
      <filename>pg_hba.conf</filename> format. Set <literal>pgpool</literal> and <literal>postgres</literal> user's authentication method to <literal>scram-sha-256</literal>.
     </para>
     <programlisting>
      host    all         pgpool           0.0.0.0/0          scram-sha-256
      host    all         postgres         0.0.0.0/0          scram-sha-256
     </programlisting>
     <para>
      The default password file name for authentication is <xref linkend="GUC-POOL-PASSWD">.
       To use <literal>scram-sha-256</literal> authentication, the decryption key to decrypt the passwords
       is required. We create the .pgpoolkey file in root user's home directory.
       <programlisting>
	[all servers]# echo 'some string' > ~/.pgpoolkey 
	[all servers]# chmod 600 ~/.pgpoolkey
       </programlisting>
     </para>
     <para>
      Execute command <command>pg_enc -m -k /path/to/.pgpoolkey -u username -p</command> to register user
      name and <literal>AES</literal> encrypted password in file <filename>pool_passwd</filename>. 
      If <filename>pool_passwd</filename> doesn't exist yet, it will be created in the same directory as
      <filename>pgpool.conf</filename>.
     </para>
     <programlisting>
      [all servers]# pg_enc -m -k /root/.pgpoolkey -u pgpool -p
      db password: [pgpool user's password]
      [all servers]# pg_enc -m -k /root/.pgpoolkey -u postgres -p
      db password: [postgres user's passowrd]

      # cat /etc/pgpool-II/pool_passwd 
      pgpool:AESheq2ZMZjynddMWk5sKP/Rw==
      postgres:AESHs/pWL5rtXy2IwuzroHfqg==
     </programlisting>
    </sect3>

    <sect3 id="example-cluster-pgpool-config-watchdog">
     <title>Watchdog Configuration</title>
     <para>
      Enable watchdog functionality on <literal>server1</literal>, <literal>server2</literal>, <literal>server3</literal>.
     </para>
     <programlisting>
      use_watchdog = on
     </programlisting>
     <para>
      Specify virtual IP address that accepts connections from clients on 
      <literal>server1</literal>, <literal>server2</literal>, <literal>server3</literal>. 
      Ensure that the IP address set to virtual IP isn't used yet.
     </para>
     <programlisting>
      delegate_IP = '192.168.137.150'
     </programlisting>
     
     <para>
      To bring up/down the virtual IP and send the ARP requests, we set <xref linkend="GUC-IF-UP-CMD">, <xref linkend="GUC-IF-DOWN-CMD"> and <xref linkend="GUC-ARPING-CMD">.
	 The network interface used in this example is "enp0s8".
     </para>
     <programlisting>
      if_up_cmd = 'ip addr add $_IP_$/24 dev enp0s8 label enp0s8:0'
      if_down_cmd = 'ip addr del $_IP_$/24 dev enp0s8'
      arping_cmd = 'arping -U $_IP_$ -w 1 -I enp0s8'
     </programlisting>
     <para>
      Set <xref linkend="GUC-IF-CMD-PATH"> and <xref linkend="GUC-ARPING-PATH"> according to the
	command path.
     </para>
     <programlisting>
      if_cmd_path = '/sbin'
      arping_path = '/usr/sbin'
     </programlisting>
     <para>
      Specify the hostname and port number of each <productname>Pgpool-II</productname> server.
     </para>
     <itemizedlist>
      <listitem>
       <para>
	<literal>server1</literal>
       </para>
       <programlisting>
	wd_hostname = 'server1'
	wd_port = 9000
       </programlisting>
      </listitem>
      <listitem>
       <para>
	<literal>server2</literal>
       </para>
       <programlisting>
	wd_hostname = 'server2'
	wd_port = 9000
       </programlisting>
      </listitem>
      <listitem>
       <para>
	<literal>server3</literal>
       </para>
       <programlisting>
	wd_hostname = 'server3'
	wd_port = 9000
       </programlisting>
      </listitem>
     </itemizedlist>

     <para>
      Specify the hostname, <productname>Pgpool-II</productname> port number, and watchdog port number of monitored <productname>Pgpool-II</productname> servers on each <productname>Pgpool-II</productname> server.
     </para>
     <itemizedlist>
      <listitem>
       <para>
	<literal>server1</literal>
       </para>
       <programlisting>
	# - Other pgpool Connection Settings -

	other_pgpool_hostname0 = 'server2'
	other_pgpool_port0 = 9999
	other_wd_port0 = 9000

	other_pgpool_hostname1 = 'server3'
	other_pgpool_port1 = 9999
	other_wd_port1 = 9000
       </programlisting>
      </listitem>
      <listitem>
       <para>
	<literal>server2</literal>
       </para>
       <programlisting>
	# - Other pgpool Connection Settings -

	other_pgpool_hostname0 = 'server1'
	other_pgpool_port0 = 9999
	other_wd_port0 = 9000

	other_pgpool_hostname1 = 'server3'
	other_pgpool_port1 = 9999
	other_wd_port1 = 9000
       </programlisting>
      </listitem>
      <listitem>
       <para>
	<literal>server3</literal>
       </para>
       <programlisting>
	# - Other pgpool Connection Settings -

	other_pgpool_hostname0 = 'server1'
	other_pgpool_port0 = 9999
	other_wd_port0 = 9000

	other_pgpool_hostname1 = 'server2'
	other_pgpool_port1 = 9999
	other_wd_port1 = 9000
       </programlisting>
      </listitem>
     </itemizedlist>

     <para>
      Specify the hostname and port number of destination for sending heartbeat signal 
      on <literal>server1</literal>, <literal>server2</literal>, <literal>server3</literal>.
     </para>
     <itemizedlist>
      <listitem>
       <para>
	<literal>server1</literal>
       </para>
       <programlisting>
	heartbeat_destination0 = 'server2'
	heartbeat_destination_port0 = 9694
	heartbeat_device0 = ''

	heartbeat_destination1 = 'server3'
	heartbeat_destination_port1 = 9694
	heartbeat_device1 = ''

       </programlisting>
      </listitem>
      <listitem>
       <para>
	<literal>server2</literal>
       </para>
       <programlisting>
	heartbeat_destination0 = 'server1'
	heartbeat_destination_port0 = 9694
	heartbeat_device0 = ''

	heartbeat_destination1 = 'server3'
	heartbeat_destination_port1 = 9694
	heartbeat_device1 = ''

       </programlisting>
      </listitem>
      <listitem>
       <para>
	<literal>server3</literal>
       </para>
       <programlisting>
	heartbeat_destination0 = 'server1'
	heartbeat_destination_port0 = 9694
	heartbeat_device0 = ''

	heartbeat_destination1 = 'server2'
	heartbeat_destination_port1 = 9694
	heartbeat_device1 = ''
       </programlisting>
      </listitem>
     </itemizedlist>
    </sect3>

    <sect3 id="example-cluster-pgpool-config-sysconfig">
     <title>/etc/sysconfig/pgpool Configuration</title>
     <para>
      If you want to ignore the <filename>pgpool_status</filename> file at startup of 
      <productname>Pgpool-II</productname>, add "- D" to the start option OPTS to 
      <filename>/etc/sysconfig/pgpool</filename>.
     </para>
     <programlisting>
      [all servers]# vi /etc/sysconfig/pgpool 
      ...
      OPTS=" -D -n"
     </programlisting>
    </sect3>

    <sect3 id="example-cluster-pgpool-config-log">
     <title>Logging</title>
     <para>
      In the example, we output <productname>Pgpool-II</productname>'s log to <literal>syslog</literal>.
     </para>
     <programlisting>
      log_destination = 'syslog'
      syslog_facility = 'LOCAL1'
     </programlisting>
     <para>
      Create <productname>Pgpool-II</productname> log file.
     </para>
     <programlisting>
      [all servers]# mkdir /var/log/pgpool-II
      [all servers]# touch /var/log/pgpool-II/pgpool.log
     </programlisting>
     <para>
      Edit config file of syslog <filename>/etc/rsyslog.conf</filename>.
     </para>
     <programlisting>
      [all servers]# vi /etc/rsyslog.conf
      ...
      *.info;mail.none;authpriv.none;cron.none;LOCAL1.none    /var/log/messages
      LOCAL1.*                                                /var/log/pgpool-II/pgpool.log
     </programlisting>
     <para>
      Setting logrotate same as <filename>/var/log/messages</filename>.
     </para>
     <programlisting>
      [all servers]# vi /etc/logrotate.d/syslog
      ...
      /var/log/messages
      /var/log/pgpool-II/pgpool.log
      /var/log/secure
     </programlisting>

     <para>
      Restart rsyslog service.
     </para>
     <programlisting>
      [all servers]# systemctl restart rsyslog
     </programlisting>
    </sect3>

    <sect3 id="example-cluster-pgpool-config-pcp">
     <title>PCP Command Configuration</title>
     <para>
      Since user authentication is required to use the <literal>PCP</literal> command, 
      specify user name and md5 encrypted password in <filename>pcp.conf</filename>.
      Here we create the encrypted password for <literal>pgpool</literal> user, and add
      "<literal>username:encrypted password</literal>" in <filename>/etc/pgpool-II/pcp.conf</filename>.
     </para>
     <programlisting>
      [all servers]# echo 'pgpool:'`pg_md5 PCP passowrd` >> /etc/pgpool-II/pcp.conf
     </programlisting>
    </sect3>

    <sect3 id="example-cluster-pgpool-config-pcppass">
     <title>.pcppass</title>
     <para>
      Since follow_master_command script has to execute PCP command without entering the
      password, we create <filename>.pcppass</filename> in the home directory of 
      <productname>Pgpool-II</productname> startup user (root user).
     </para>
     <programlisting>
      [all servers]# echo 'localhost:9898:pgpool:pgpool' > ~/.pcppass
      [all servers]# chmod 600 ~/.pcppass
     </programlisting>
     <para>
      The settings of <productname>Pgpool-II</productname> is completed. 
     </para>
    </sect3>

   </sect2>

   <sect2 id="example-cluster-start-stop">
    <title>Starting/Stopping Pgpool-II</title>
    <para>
     Next we start <productname>Pgpool-II</productname>. Before starting 
     <productname>Pgpool-II</productname>, please start 
     <productname>PostgreSQL</productname> servers first. 
     Also, when stopping <productname>PostgreSQL</productname>, it is necessary to 
     stop Pgpool-II first.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Starting <productname>Pgpool-II</productname>
      </para>
      <programlisting>
       # systemctl start pgpool.service
      </programlisting>
     </listitem>
     <listitem>
      <para>
       Stopping <productname>Pgpool-II</productname>
      </para>
      <programlisting>
       # systemctl stop pgpool.service
      </programlisting>
     </listitem>
    </itemizedlist>
   </sect2>

   <sect2 id="example-cluster-try">
    <title>How to use</title>
    <para>
     Let's start to use <productname>Pgpool-II</productname>. 
     First, let's start <productname>Pgpool-II</productname> on <literal>server1</literal>, 
     <literal>server2</literal>, <literal>server3</literal> by using the following command.
    </para>
    <programlisting>
     # systemctl start pgpool.service
    </programlisting>

    <sect3 id="example-cluster-try-standby">
     <title>Set up PostgreSQL standby server</title>
     <para>
      First, we should set up <productname>PostgreSQL</productname> standby server by 
      using <productname>Pgpool-II</productname> online recovery functionality. Ensure 
      that <filename>recovery_1st_stage</filename> and <filename>pgpool_remote_start</filename> 
      scripts used by <command>pcp_recovery_node</command> command are in database 
      cluster directory of <productname>PostgreSQL</productname> primary server (<literal>server1</literal>).
     </para>
     <programlisting>
      # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 1
      Password: 
      pcp_recovery_node -- Command Successful

      # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 2
      Password: 
      pcp_recovery_node -- Command Successful
     </programlisting>
     <para>
      After executing <command>pcp_recovery_node</command> command, 
      verify that <literal>server2</literal> and <literal>server3</literal>
      are started as <productname>PostgreSQL</productname> standby server.
     </para>
     <programlisting>
      # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
      node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | last_status_change  
      ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+---------------------
      0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 | 2019-02-18 11:26:31
      1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | 2019-02-18 11:27:49
      2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 11:27:49
     </programlisting>
    </sect3>

    <sect3 id="example-cluster-try-watchdog">
     <title>Switching active/standby watchdog</title>
     <para>
      Confirm the watchdog status by using <command>pcp_watchdog_info</command>. The <command>Pgpool-II</command> server which is started first run as <literal>MASTER</literal>.
     </para>
     <programlisting>
      # pcp_watchdog_info -h 192.168.137.150 -p 9898 -U pgpool
      Password: 
      3 YES server1:9999 Linux server1 server1

      server1:9999 Linux server1 server1 9999 9000 4 MASTER  #The Pgpool-II server started first becames "MASTER".
      server2:9999 Linux server2 server2 9999 9000 7 STANDBY #run as standby
      server3:9999 Linux server3 server3 9999 9000 7 STANDBY #run as standby
     </programlisting>
     <para>
      Stop active server <literal>server1</literal>, then <literal>server2</literal> or 
      <literal>server3</literal> will be promoted to active server. To stop 
      <literal>server1</literal>, we can stop <productname>Pgpool-II</productname> 
      service or shutdown the whole system. Here, we stop <productname>Pgpool-II</productname> service.
     </para>
     <programlisting>
      [server1]# systemctl stop pgpool.service

      # pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
      Password: 
      3 YES server2:9999 Linux server2 server2

      server2:9999 Linux server2 server2 9999 9000 4 MASTER     #server2 is promoted to MASTER
      server1:9999 Linux server1 server1 9999 9000 10 SHUTDOWN  #server1 is stopped
      server3:9999 Linux server3 server3 9999 9000 7 STANDBY    #server3 runs as STANDBY
     </programlisting>
     <para>
      Start <productname>Pgpool-II</productname> (<literal>server1</literal>) which we have stopped again, and verify that <literal>server1</literal> runs as a standby.
     </para>
     <programlisting>
      [server1]# systemctl start pgpool.service

      [server1]# pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
      Password: 
      3 YES server2:9999 Linux server2 server2

      server2:9999 Linux server2 server2 9999 9000 4 MASTER
      server1:9999 Linux server1 server1 9999 9000 7 STANDBY
      server3:9999 Linux server3 server3 9999 9000 7 STANDBY
     </programlisting>
    </sect3>

    <sect3 id="example-cluster-try-failover">
     <title>Failover</title>
     <para>
      First, use <command>psql</command> to connect to <productname>PostgreSQL</productname> via virtual IP, and verify the backend information.
     </para>
     <programlisting>
      # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
      node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | last_status_change  
      ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+---------------------
      0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 | 2019-02-18 13:08:02
      1       | server2  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 13:21:56
      2       | server3  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | 2019-02-18 13:21:56
     </programlisting>
     <para>
      Next, stop primary <productname>PostgreSQL</productname> server 
      <literal>server1</literal>, and verify automatic failover.
     </para>
     <programlisting>
      [server1]$ pg_ctl -D /var/lib/pgsql/11/data -m immediate stop
     </programlisting>
     <para>
      After stopping <productname>PostgreSQL</productname> on <literal>server1</literal>,
      failover occurs and <productname>PostgreSQL</productname> on 
      <literal>server2</literal> becomes new primary DB.
     </para>
     <programlisting>
      # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
      node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | last_status_change  
      ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+---------------------
      0       | server1  | 5432 | down   | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 13:22:25
      1       | server2  | 5432 | up     | 0.333333  | primary | 0          | true              | 0                 | 2019-02-18 13:22:25
      2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 13:22:28
     </programlisting>
     <para>
      <literal>server3</literal> is running as standby of new primary <literal>server2</literal>.
     </para>

     <programlisting>
      [server3]# psql -h server3 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
      pg_is_in_recovery 
      -------------------
      t

      [server2]# su - postgres
      $ psql
      postgres=# select pg_is_in_recovery();
      pg_is_in_recovery 
      -------------------
      f

      postgres=# select * from pg_stat_replication;
      -[ RECORD 1 ]----+------------------------------
      pid              | 11915
      usesysid         | 16385
      usename          | repl
      application_name | walreceiver
      client_addr      | 192.168.137.103
      client_hostname  | 
      client_port      | 37834
      backend_start    | 2019-02-18 13:22:27.472038+09
      backend_xmin     | 
      state            | streaming
      sent_lsn         | 0/8E000060
      write_lsn        | 0/8E000060
      flush_lsn        | 0/8E000060
      replay_lsn       | 0/8E000060
      write_lag        | 
      flush_lag        | 
      replay_lag       | 
      sync_priority    | 0
      sync_state       | async
     </programlisting>
    </sect3>

    <sect3 id="example-cluster-try-online-recovery">
     <title>Online Recovery</title>
     <para>
      Here, we use <productname>Pgpool-II</productname> online recovery functionality to
      restore <literal>server1</literal> (old primary server) as a standby. Before 
      restoring the old primary server, please ensure that 
      <filename>recovery_1st_stage</filename> and <filename>pgpool_remote_start</filename> scripts 
      exist in database cluster directory of current primary server <literal>server2</literal>.
     </para>
     <programlisting>
      # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 0
      Password: 
      pcp_recovery_node -- Command Successful
     </programlisting>
     <para>
      Then verify that <literal>server1</literal> is started as a standby.
     </para>
     <programlisting>
      # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
      node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | last_status_change  
      ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+---------------------
      0       | server1  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 13:27:44
      1       | server2  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 | 2019-02-18 13:22:25
      2       | server3  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | 2019-02-18 13:22:28
     </programlisting>
    </sect3>
   </sect2>
  </sect1>

  <sect1 id="example-AWS">
   <title>AWS Configuration Example</title>

   <para>
    This tutorial explains the simple way to try "Watchdog"
    on <ulink url="https://aws.amazon.com/">AWS</ulink> and using
    the <ulink url="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html">
     Elastic IP Address</ulink> as the Virtual IP for the high availability solution.
    <note>
     <para>
      You can use watchdog with <productname>
       Pgpool-II</productname> in any mode: replication mode,
      master/slave mode and raw mode.
     </para>
    </note>
   </para>

   <sect2 id="example-AWS-setup">
    <title>AWS Setup</title>
    <para>
     For this example, we will use two node <productname>
      Pgpool-II</productname> watchdog cluster. So we will set up two
     Linux Amazon EC2 instances and one Elastic IP address.
     So for this example, do the following steps:
    </para>
    <itemizedlist>

     <listitem>
      <para>
       Launch two Linux Amazon EC2 instances. For this example, we name these
       instances as "instance-1" and "instance-2"
      </para>
     </listitem>

     <listitem>
      <para>
       Configure the security group for the instances and allow inbound traffic
       on ports used by pgpool-II and watchdog.
      </para>
     </listitem>

     <listitem>
      <para>
       Install the <productname>Pgpool-II</productname> on both instances.
      </para>
     </listitem>

     <listitem>
      <para>
       Allocate an Elastic IP address.
       For this example, we will use "35.163.178.3" as an Elastic IP address"
      </para>
     </listitem>

    </itemizedlist>

   </sect2>

   <sect2 id="example-AWS-pgpool-config">
    <title><productname>Pgpool-II</productname> configurations</title>
    <para>
     Mostly the <productname>Pgpool-II</productname> configurations for this
     example will be same as in the <xref linkend="example-watchdog">, except the
      <xref linkend="guc-delegate-ip"> which we will not set in this example instead
       we will use <xref linkend="guc-wd-escalation-command"> and
	<xref linkend="guc-wd-de-escalation-command"> to switch the
	 Elastic IP address to the master/Active <productname>Pgpool-II</productname> node.
    </para>

    <sect3 id="example-AWS-pgpool-config-instance-1">
     <title><productname>Pgpool-II</productname> configurations on Instance-1</title>
     <para>

      <programlisting>
       use_watchdog = on
       delegate_IP = ''
       wd_hostname = 'instance-1-private-ip'
       other_pgpool_hostname0 = 'instance-2-private-ip'
       other_pgpool_port0 = 9999
       other_wd_port0 = 9000
       wd_escalation_command = '$path_to_script/aws-escalation.sh'
       wd_de_escalation_command = '$path_to_script/aws-de-escalation.sh'
      </programlisting>

     </para>
    </sect3>

    <sect3 id="example-AWS-pgpool-config-instance-2">
     <title><productname>Pgpool-II</productname> configurations on Instance-2</title>
     <para>

      <programlisting>
       use_watchdog = on
       delegate_IP = ''
       wd_hostname = 'instance-2-private-ip'
       other_pgpool_hostname0 = 'instance-1-private-ip'
       other_pgpool_port0 = 9999
       other_wd_port0 = 9000
       wd_escalation_command = '$path_to_script/aws-escalation.sh'
       wd_de_escalation_command = '$path_to_script/aws-de-escalation.sh'
      </programlisting>

     </para>
    </sect3>
   </sect2>

   <sect2 id="example-AWS-pgpool-aws-escalation-instance">
    <title>escalation and de-escalation Scripts</title>
    <para>
     Create the aws-escalation.sh and aws-de-escalation.sh scripts on both
     instances and point the <xref linkend="guc-wd-escalation-command"> and
      <xref linkend="guc-wd-de-escalation-command"> to the respective scripts.
    </para>

    <note>
     <para>
      You may need to configure the AWS CLI first on all AWS instances
      to enable the execution of commands used by wd-escalation.sh and wd-de-escalation.sh.
      See <ulink url="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html">configure AWS CLI</ulink>
     </para>
    </note>

    <sect3 id="example-AWS-pgpool-aws-escalation-script">
     <title>escalation script</title>

     <para>
      This script will be executed by the watchdog
      to assign the Elastic IP on the instance when the watchdog becomes the active/master node.
      Change the INSTANCE_ID and ELASTIC_IP values as per your AWS setup values.
     </para>
     <para>
      <emphasis>aws-escalation.sh:</emphasis>
      <programlisting>
       #! /bin/sh

       ELASTIC_IP=35.163.178.3
       # replace it with the Elastic IP address you
       # allocated from the aws console
       INSTANCE_ID=i-0a9b64e449b17ed4b
       # replace it with the instance id of the Instance
       # this script is installed on

       echo "Assigning Elastic IP $ELASTIC_IP to the instance $INSTANCE_ID"
       # bring up the Elastic IP
       aws ec2 associate-address --instance-id $INSTANCE_ID --public-ip $ELASTIC_IP

       exit 0
      </programlisting>

     </para>

    </sect3>
    <sect3 id="example-AWS-pgpool-aws-de-escalation-script">
     <title>de-escalation script</title>

     <para>
      This script will be executed by watchdog
      to remove the Elastic IP from the instance when the watchdog resign from the active/master node.
     </para>
     <para>
      <emphasis>aws-de-escalation.sh:</emphasis>
      <programlisting>
       #! /bin/sh

       ELASTIC_IP=35.163.178.3
       # replace it with the Elastic IP address you
       # allocated from the aws console

       echo "disassociating the Elastic IP $ELASTIC_IP from the instance"
       # bring down the Elastic IP
       aws ec2 disassociate-address --public-ip $ELASTIC_IP
       exit 0
      </programlisting>
     </para>
    </sect3>

    <bibliography>
     <title>AWS Command References</title>

     <biblioentry>
      <biblioset relation="article">
       <title><ulink url="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html">Configure AWS CLI</ulink></title>
      </biblioset>
      <biblioset relation="book">
       <title>AWS Documentation: Configuring the AWS Command Line Interface</title>
      </biblioset>
     </biblioentry>

     <biblioentry>
      <biblioset relation="article">
       <title><ulink url="http://docs.aws.amazon.com/cli/latest/reference/ec2/associate-address.html">associate-address</ulink></title>
      </biblioset>
      <biblioset relation="book">
       <title>AWS Documentation: associate-address reference</title>
      </biblioset>
     </biblioentry>

     <biblioentry>
      <biblioset relation="article">
       <title><ulink url="http://docs.aws.amazon.com/cli/latest/reference/ec2/disassociate-address.html">disassociate-address</ulink></title>
      </biblioset>
      <biblioset relation="book">
       <title>AWS Documentation: disassociate-address reference</title>
      </biblioset>
     </biblioentry>

    </bibliography>
   </sect2>

   <sect2 id="example-AWS-try">
    <title>Try it out</title>
    <para>
     Start <productname>Pgpool-II</productname> on each server with "-n" switch
     and redirect log messages to the pgpool.log file.
     The log message of master/active <productname>Pgpool-II</productname> node
     will show the message of Elastic IP assignment.
     <programlisting>
      LOG:  I am the cluster leader node. Starting escalation process
      LOG:  escalation process started with PID:23543
      LOG:  watchdog: escalation started
      <emphasis>
       Assigning Elastic IP 35.163.178.3 to the instance i-0a9b64e449b17ed4b
       {
       "AssociationId": "eipassoc-39853c42"
       }
      </emphasis>
      LOG:  watchdog escalation successful
      LOG:  watchdog escalation process with pid: 23543 exit with SUCCESS.
     </programlisting>
    </para>

    <para>
     Confirm to ping to the Elastic IP address.
     <programlisting>
      [user@someserver]$ ping 35.163.178.3
      PING 35.163.178.3 (35.163.178.3) 56(84) bytes of data.
      64 bytes from 35.163.178.3: icmp_seq=1 ttl=64 time=0.328 ms
      64 bytes from 35.163.178.3: icmp_seq=2 ttl=64 time=0.264 ms
      64 bytes from 35.163.178.3: icmp_seq=3 ttl=64 time=0.412 ms
     </programlisting>
    </para>

    <para>
     Try to connect <productname>PostgreSQL</productname> by "psql -h ELASTIC_IP -p port".
     <programlisting>
      [user@someserver]$ psql -h 35.163.178.3 -p 9999 -l
     </programlisting>
    </para>
   </sect2>

   <sect2 id="example-AWS-vip-switch">
    <title>Switching Elastic IP</title>
    <para>
     To confirm if the Standby server acquires the Elastic IP when the
     Active server becomes unavailable, Stop the <productname>Pgpool-II</productname>
     on the Active server. Then, the Standby server should start using the Elastic IP address,
     And the <productname>Pgpool-II</productname> log will show the below messages.

     <programlisting>
      <emphasis>
       LOG:  remote node "172.31.2.94:9999 [Linux ip-172-31-2-94]" is shutting down
       LOG:  watchdog cluster has lost the coordinator node
      </emphasis>
      LOG:  watchdog node state changed from [STANDBY] to [JOINING]
      LOG:  watchdog node state changed from [JOINING] to [INITIALIZING]
      LOG:  I am the only alive node in the watchdog cluster
      HINT:  skipping stand for coordinator state
      LOG:  watchdog node state changed from [INITIALIZING] to [MASTER]
      LOG:  I am announcing my self as master/coordinator watchdog node
      LOG:  I am the cluster leader node
      DETAIL:  our declare coordinator message is accepted by all nodes
      LOG:  I am the cluster leader node. Starting escalation process
      LOG:  escalation process started with PID:23543
      LOG:  watchdog: escalation started
      <emphasis>
       Assigning Elastic IP 35.163.178.3 to the instance i-0dd3e60734a6ebe14
       {
       "AssociationId": "eipassoc-39853c42"
       }
      </emphasis>
      LOG:  watchdog escalation successful
      LOG:  watchdog escalation process with pid: 61581 exit with SUCCESS.
     </programlisting>
     Confirm to ping to the Elastic IP address again.
     <programlisting>
      [user@someserver]$ ping 35.163.178.3
      PING 35.163.178.3 (35.163.178.3) 56(84) bytes of data.
      64 bytes from 35.163.178.3: icmp_seq=1 ttl=64 time=0.328 ms
      64 bytes from 35.163.178.3: icmp_seq=2 ttl=64 time=0.264 ms
      64 bytes from 35.163.178.3: icmp_seq=3 ttl=64 time=0.412 ms
     </programlisting>
    </para>
    <para>
     Try to connect <productname>PostgreSQL</productname> by "psql -h ELASTIC_IP -p port".
     <programlisting>
      [user@someserver]$ psql -h 35.163.178.3 -p 9999 -l
     </programlisting>
    </para>
   </sect2>
  </sect1>

  <sect1 id="example-Aurora">
   <title>Aurora Configuration Example</title>

   <para>
    <productname>Amazon Aurora for PostgreSQL
     Compatibility</productname> (Aurora) is a managed service for
    <productname>PostgreSQL</productname>. From user's point of
    view, <productname>Aurora</productname> can be regarded as a
    streaming replication cluster with some exceptions. First,
    fail over and online recovery are managed
    by <productname>Aurora</productname>. So you don't need to
    set <xref linkend="guc-failover-command">, <xref linkend="guc-follow-master-command">,
      and recovery related parameters. In this section we explain
      how to set up <productname>Pgpool-II</productname> for Aurora.
   </para>

   <sect2 id="example-Aurora-config">
    <title>Setting pgpool.conf for Aurora</title>
    <para>
     <itemizedlist>
      <listitem>
       <para>
	Create <filename>pgpool.conf</filename>
	from <filename>pgpool.conf.sample-stream</filename>.
       </para>
      </listitem>
      <listitem>
       <para>
	Set <xref linkend="guc-sr-check-period"> to 0 to
	 disable streaming replication delay checking.  This
	 is because <productname>Aurora</productname> does
	 not provide necessary functions to check the
	 replication delay.
       </para>
      </listitem>
      <listitem>
       <para>
	Enable <xref linkend="guc-enable-pool-hba"> to on so
	 that md5 authentication is enabled
	 (<productname>Aurora</productname> always use md5
	 authentication).
       </para>
      </listitem>
      <listitem>
       <para>
	Create <filename>pool_password</filename>. See <xref linkend="auth-md5">
	 for more details.
       </para>
      </listitem>
      <listitem>
       <para>
	Set <xref linkend="guc-backend-hostname">0 for the
	 Aurora writer node.  Set
	 other <xref linkend="guc-backend-hostname"> for the
	  Aurora reader node.  Set
	  appropriate <xref linkend="guc-backend-weight"> as
	   usual. You don't need to
	   set <xref linkend="guc-backend-data-directory">
       </para>
      </listitem>
      <listitem>
       <para>
	Set <varname>ALWAYS_MASTER</varname> flag to
	the <xref linkend="guc-backend-flag">
	 for <xref linkend="guc-backend-hostname">0.
       </para>
      </listitem>
      <listitem>
       <para>
	Enable health checking.
	Set <xref linkend="guc-health-check-period"> to 5.
	 Set <xref linkend="guc-health-check-user">, <xref linkend="guc-health-check-password">,
	   <xref linkend="guc-health-check-user"> and
	    <xref linkend="guc-health-check-database"> to appropriate values.
	     Enable health check retry.
	     <productname>Aurora</productname> shutdowns all DB nodes while switching
	     over or failover. If the retry is not performed,
	     <productname>Pgpool-II</productname> thinks that all DB nodes are in down status
	     so that it is required to restart <productname>Pgpool-II</productname>.
	     Set <xref linkend="guc-health-check-max-retries"> to 20.
	      Set <xref linkend="guc-health-check-retry-delay"> to 1 to avoid the problem.
       </para>
      </listitem>
      <listitem>
       <para>
	Disable <xref linkend="guc-failover-on-backend-error">
	 to avoid failover when connecting to the backend or
	 detecting errors on backend side while executing
	 queries for the same reasons above.
       </para>
      </listitem>
     </itemizedlist>
    </para>
   </sect2>
  </sect1>

 </chapter>
</part>
