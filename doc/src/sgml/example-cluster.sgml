<sect1 id="example-cluster">
 <title><productname>Pgpool-II</productname> + Watchdog Setup Example</title>
 <para>
  This section shows an example of streaming replication configuration using
  <productname>Pgpool-II</productname>. In this example, we use 3
  <productname>Pgpool-II</productname> servers to manage <productname>PostgreSQL</productname>
  servers to create a robust cluster system and avoid the single point of failure or split brain.
 </para>
 <para>
  <productname>PostgreSQL</productname> 16 is used in this configuration example.
  All scripts have been tested with <productname>PostgreSQL</productname> 10 and later.
 </para>
 <sect2 id="example-cluster-requirement">
  <title>Requirements</title>
  <para>
   We assume that all the Pgpool-II servers and the <productname>PostgreSQL</productname> servers are in the same subnet.
  </para>
 </sect2>

 <sect2 id="example-cluster-structure">
  <title>Cluster System Configuration</title>
  <para>
   We use three servers with Rocky Linux 8 installed and
   the hostnames of the three servers are <literal>server1</literal>
   <literal>server2</literal> and <literal>server3</literal> respectively.
   We install <productname>PostgreSQL</productname> and <productname>Pgpool-II</productname> on each server.
  </para>
  <para>
   <figure>
    <title>Cluster System Configuration</title>
    <mediaobject>
     <imageobject>
      <imagedata fileref="cluster_40.gif">
     </imageobject>
    </mediaobject>
   </figure>
  </para>
  <note>
   <para>
    The roles of <literal>Leader</literal>, <literal>Standby</literal>, <literal>Primary</literal>,
    <literal>Standby</literal> are not fixed and may be changed by further operations.
   </para>
  </note>
  <table id="example-cluster-table-ip">
   <title>Hostname and IP address</title>
   <tgroup cols="3">
    <thead>
     <row>
      <entry>Hostname</entry>
      <entry>IP Address</entry>
      <entry>Virtual IP</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>server1</entry>
      <entry>192.168.100.51</entry>
      <entry morerows="2">192.168.100.50</entry>
     </row>
     <row>
      <entry>server2</entry>
      <entry>192.168.100.52</entry>
     </row>
     <row>
      <entry>server3</entry>
      <entry>192.168.100.53</entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <table id="example-cluster-table-postgresql-config">
   <title>PostgreSQL version and Configuration</title>
   <tgroup cols="3">
    <thead>
     <row>
      <entry>Item</entry>
      <entry>Value</entry>
      <entry>Detail</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>PostgreSQL Version</entry>
      <entry>16.0</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry>port</entry>
      <entry>5432</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry>$PGDATA</entry>
      <entry>/var/lib/pgsql/16/data</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry>Archive mode</entry>
      <entry>on</entry>
      <entry>/var/lib/pgsql/archivedir</entry>
     </row>
     <row>
      <entry>Replication Slots</entry>
      <entry>Enabled</entry>
      <entry>In this configuration example, replication slots are
       automatically created or deleted in the scripts which are executed
       during failover or online recovery. 
       These scripts use the hostname specified in backend_hostnameX as
       the replication slot name.
       See <xref linkend="example-cluster-table-sample-scripts"> for
       more information about the scripts.</entry>
     </row>
     <row>
      <entry>Async/Sync Replication</entry>
      <entry>Async</entry>
      <entry>-</entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <table id="example-cluster-table-pgpool-config">
   <title>Pgpool-II version and Configuration</title>
   <tgroup cols="3">
    <thead>
     <row>
      <entry>Item</entry>
      <entry>Value</entry>
      <entry>Detail</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>Pgpool-II Version</entry>
      <entry>4.5.0</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry morerows='3'>port</entry>
      <entry>9999</entry>
      <entry>Pgpool-II accepts connections</entry>
     </row>
     <row>
      <entry>9898</entry>
      <entry>PCP process accepts connections</entry>
     </row>
     <row>
      <entry>9000</entry>
      <entry>watchdog accepts connections</entry>
     </row>
     <row>
      <entry>9694</entry>
      <entry>UDP port for receiving Watchdog's heartbeat signal</entry>
     </row>
     <row>
      <entry>Config file</entry>
      <entry>/etc/pgpool-II/pgpool.conf</entry>
      <entry>Pgpool-II config file</entry>
     </row>
     <row>
      <entry>User running Pgpool-II</entry>
      <entry>postgres (Pgpool-II 4.1 or later)</entry>
      <entry>Pgpool-II 4.0 or before, the default user running Pgpool-II is root</entry>
     </row>
     <row>
      <entry>Running mode</entry>
      <entry>streaming replication mode</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry>Watchdog</entry>
      <entry>on</entry>
      <entry>Life check method: heartbeat</entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <table id="example-cluster-table-sample-scripts">
   <title>Various sample scripts included in rpm package</title>
   <tgroup cols="3">
    <thead>
     <row>
      <entry>Feature</entry>
      <entry>Script</entry>
      <entry>Detail</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry morerows='1'>Failover</entry>
      <entry><ulink url="https://raw.githubusercontent.com/pgpool/pgpool2/refs/heads/V4_5_STABLE/src/sample/scripts/failover.sh.sample">/etc/pgpool-II/sample_scripts/failover.sh.sample</ulink></entry>
      <entry>Run by <xref linkend="GUC-FAILOVER-COMMAND"> to perform failover</entry>
     </row>
     <row>
      <entry><ulink url="https://raw.githubusercontent.com/pgpool/pgpool2/refs/heads/V4_5_STABLE/src/sample/scripts/follow_primary.sh.sample">/etc/pgpool-II/sample_scripts/follow_primary.sh.sample</ulink></entry>
      <entry>Run by <xref linkend="GUC-FOLLOW-PRIMARY-COMMAND"> to synchronize the Standby with the new Primary after failover.</entry>
     </row>
     <row>
      <entry morerows='1'>Online recovery</entry>
      <entry><ulink url="https://raw.githubusercontent.com/pgpool/pgpool2/refs/heads/V4_5_STABLE/src/sample/scripts/recovery_1st_stage.sample">/etc/pgpool-II/sample_scripts/recovery_1st_stage.sample</ulink></entry>
      <entry>Run by <xref linkend="GUC-RECOVERY-1ST-STAGE-COMMAND"> to recovery a Standby node</entry>
     </row>
     <row>
      <entry><ulink url="https://raw.githubusercontent.com/pgpool/pgpool2/refs/heads/V4_5_STABLE/src/sample/scripts/pgpool_remote_start.sample">/etc/pgpool-II/sample_scripts/pgpool_remote_start.sample</ulink></entry>
      <entry>Run after <xref linkend="GUC-RECOVERY-1ST-STAGE-COMMAND"> to start the Standby node</entry>
     </row>
     <row>
      <entry morerows='1'>Watchdog</entry>
      <entry><ulink url="https://raw.githubusercontent.com/pgpool/pgpool2/refs/heads/V4_5_STABLE/src/sample/scripts/escalation.sh.sample">/etc/pgpool-II/sample_scripts/escalation.sh.sample</ulink></entry>
      <entry>Optional Configuration. Run by <xref linkend="guc-wd-escalation-command"> to switch the Leader/Standby Pgpool-II safely</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
  <para>
   The above scripts are included in the RPM package and can be customized as needed.
  </para>
 </sect2>

 <sect2 id="example-cluster-installation">
  <title>Installation</title>
  <para>
   In this example, we install <productname>Pgpool-II</productname> and <productname>PostgreSQL</productname> RPM packages with YUM.
  </para>

  <para>
   Install <productname>PostgreSQL</productname> from <productname>PostgreSQL</productname> YUM repository.
  </para>
  <programlisting>
[all servers]# dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm
[all servers]# dnf -qy module disable postgresql
[all servers]# dnf install -y postgresql16-server
  </programlisting>
  <para>
   Since <productname>Pgpool-II</productname> related packages are also included in <productname>PostgreSQL</productname> YUM repository,
   add the "exclude" settings to <filename>/etc/yum.repos.d/pgdg-redhat-all.repo</filename>
   so that <productname>Pgpool-II</productname> is not installed from <productname>PostgreSQL</productname> YUM repository. 
  </para>
  <programlisting>
[all servers]# vi /etc/yum.repos.d/pgdg-redhat-all.repo
  </programlisting>
  <para>
   The following is a setting example of <filename>/etc/yum.repos.d/pgdg-redhat-all.repo</filename>. 
  </para>
  <programlisting>
[pgdg-common]
...
exclude=pgpool*

[pgdg16]
...
exclude=pgpool*

[pgdg15]
...
exclude=pgpool*

[pgdg14]
...
exclude=pgpool*

[pgdg13]
...
exclude=pgpool*

[pgdg12]
...
exclude=pgpool*

[pgdg11]
...
exclude=pgpool*
  </programlisting>

  <para>
   Install <productname>Pgpool-II</productname> from Pgpool-II YUM repository.
  </para>
  <programlisting>
[all servers]# dnf install -y https://www.pgpool.net/yum/rpms/4.5/redhat/rhel-8-x86_64/pgpool-II-release-4.5-1.noarch.rpm
[all servers]# dnf install -y pgpool-II-pg16-*
  </programlisting>
 </sect2>

 <sect2 id="example-cluster-pre-setup">
  <title>Before Starting</title>
  <para>
   Before you start the configuration process, please check the following prerequisites.
  </para>

  <sect3 id="example-cluster-before-starting-primary">
   <title>Setting up streaming replication on Primary</title>
   <para>
    Set up <productname>PostgreSQL</productname> streaming replication on the primary server.
    In this example, we use WAL archiving.
   </para>
   <para>
    First, we create the directory <filename>/var/lib/pgsql/archivedir</filename> to store
    <acronym>WAL</acronym> segments on all servers. In this example, only Primary node archives
    <acronym>WAL</acronym> locally.
   </para>
   <programlisting>
[all servers]# su - postgres
[all servers]$ mkdir /var/lib/pgsql/archivedir
   </programlisting>

   <para>
    Initialize <productname>PostgreSQL</productname> on the primary server.
   </para>
   <programlisting>
[server1]# su - postgres
[server1]$ /usr/pgsql-16/bin/initdb -D $PGDATA
    </programlisting>

    <para>
     Then we edit the configuration file <filename>$PGDATA/postgresql.conf</filename>
     on <literal>server1</literal> (primary) as follows. Enable <literal>wal_log_hints</literal>
     to use <literal>pg_rewind</literal>. 
     Since the Primary may become a Standby later, we set <varname>hot_standby = on</varname>.
    </para>
    <programlisting>
listen_addresses = '*'
archive_mode = on
archive_command = 'cp "%p" "/var/lib/pgsql/archivedir/%f"'
max_wal_senders = 10
max_replication_slots = 10
wal_level = replica
hot_standby = on
wal_log_hints = on
    </programlisting>
    <para>
     Start PostgreSQL primary server on <literal>server1</literal>.
    </para>
   <programlisting>
[server1]# su - postgres
[server1]$ /usr/pgsql-16/bin/pg_ctl start -D $PGDATA
    </programlisting>
  </sect3>

  <sect3 id="example-cluster-before-starting-standby">
   <title>Setting up streaming replication on Standby</title>
   <para>
    There are multiple methods to setup a standby server, such as:
    <itemizedlist>
     <listitem>
      <para>
       use pg_basebackup to backup the data directory of the primary from the standby.
      </para>
     </listitem>
     <listitem>
      <para>
       use <productname>Pgpool-II</productname>'s online recovery feature
       (<xref linkend="runtime-online-recovery">) to automatically
       setup a standby server.
      </para>
     </listitem>
    </itemizedlist>
   </para>
   <para>
    In this example, we use <productname>Pgpool-II</productname>'s
    online recovery to setup the standby server 
    in section <xref linkend="example-cluster-verify-standby">
    after the configuration of <productname>Pgpool-II</productname>
    is completed.
   </para>
  </sect3>

  <sect3 id="example-cluster-before-starting-users">
   <title>Setting up PostgreSQL users</title>
   <para>
    A PostgreSQL user is required to use <productname>Pgpool-II</productname>'s
    health check and replication delay check features.
    Because of the security reasons, we create a dedicated user named
    <literal>pgpool</literal> for streaming replication delay check and
    health check.
    And create a dedicated user named <literal>repl</literal> for replication.
    Because online recovery feature requires superuser privilege,
    we use <literal>postgres</literal> user here.
    </para>
    <para>
     Since <productname>Pgpool-II</productname> 4.0,
     <acronym>scram-sha-256</acronym> authentication is supported. 
     This configuration example uses <acronym>scram-sha-256</acronym>
     authentication method. 
     First, set <literal>password_encryption = 'scram-sha-256'</literal>
     and then created the users.
    </para>

    <table id="example-cluster-user">
     <title>Users</title>
     <tgroup cols="3">
      <thead>
       <row>
	    <entry>User Name</entry>
	    <entry>Password</entry>
	    <entry>Detail</entry>
       </row>
      </thead>
      <tbody>
       <row>
	    <entry>repl</entry>
	    <entry>repl</entry>
	    <entry>PostgreSQL replication user</entry>
       </row>
       <row>
	    <entry>pgpool</entry>
	    <entry>pgpool</entry>
	    <entry>Pgpool-II health check (<xref linkend="GUC-HEALTH-CHECK-USER">) and replication delay check (<xref linkend="GUC-SR-CHECK-USER">) user</entry>
       </row>
       <row>
	<entry>postgres</entry>
	<entry>postgres</entry>
	<entry>User running online recovery</entry>
       </row>
      </tbody>
     </tgroup>
    </table>

    <programlisting>
[server1]# psql -U postgres -p 5432
postgres=# SET password_encryption = 'scram-sha-256';
postgres=# CREATE ROLE pgpool WITH LOGIN;
postgres=# CREATE ROLE repl WITH REPLICATION LOGIN;
postgres=# \password pgpool
postgres=# \password repl
postgres=# \password postgres
    </programlisting>

    <para>
     To show <literal>replication_state</literal> and
     <literal>replication_sync_state</literal> column in
     <xref linkend="SQL-SHOW-POOL-NODES"> command result, role
     <literal>pgpool</literal> needs to be PostgreSQL superuser or in
     <literal>pg_monitor</literal> group (<productname>Pgpool-II</productname> 4.1 or later).
     Grant <literal>pg_monitor</literal>
     to <literal>pgpool</literal>:
    </para>
    <programlisting>
GRANT pg_monitor TO pgpool;
    </programlisting>
    <note>
     <para>
      If you plan to use <xref linkend="guc-detach-false-primary">(<productname>Pgpool-II</productname> 4.0 or later),
      role "pgpool" needs to be <productname>PostgreSQL</productname> superuser
      or in <literal>pg_monitor</literal> group to use this feature.
     </para>
    </note>
    <para>
     Assuming that all the <productname>Pgpool-II</productname> servers and the 
     <productname>PostgreSQL</productname> servers are in the same subnet and edit <filename>pg_hba.conf</filename> to 
     enable <literal>scram-sha-256</literal> authentication method.
    </para>
    <programlisting>
host    all             pgpool             samenet                 scram-sha-256
host    all             postgres           samenet                 scram-sha-256
host    replication     repl               samenet                 scram-sha-256
    </programlisting>
  </sect3>

  <sect3 id="example-cluster-before-starting-ssh">
   <title>Setting up SSH public key authentication</title>
   <para>
    To use the automated failover and online recovery of <productname>Pgpool-II</productname>, 
    it is required to configure <emphasis>SSH public key authentication
    (passwordless SSH login)</emphasis> to all backend servers using
    <literal>postgres</literal> user (the default user Pgpool-II is running as.
    Pgpool-II 4.0 or before, the default user is <literal>root</literal>).
   </para>
   <para>
    Execute the following command on all servers to generate a key pair using
    the RSA algorithm. In this example, we assume that the generated key file
    name is <literal>id_rsa_pgpool</literal>.
   </para>
   <programlisting>
[all servers]# su - postgres
[all servers]$ mkdir ~/.ssh
[all servers]$ chmod 700 ~/.ssh
[all servers]$ cd ~/.ssh
[all servers]$ ssh-keygen -t rsa -f id_rsa_pgpool
   </programlisting>
   <para>
    Then add the public key <filename>id_rsa_pgpool.pub</filename> to
    <filename>/var/lib/pgsql/.ssh/authorized_keys</filename> file
    on each server.
   </para>
   <para>
    After setting SSH, make sure that you can run
    <command>ssh postgres@serverX -i ~/.ssh/id_rsa_pgpool</command> command
    as <literal>postgres</literal> user to login to each server
    without entering a password. 
   </para>

   <note>
    <para>
     If you failed to login using SSH public key authentication, please check the following:
     <itemizedlist>
      <listitem>
       <para>
        Ensure the public key authentication option <literal>PubkeyAuthentication</literal> are allowed in <filename>/etc/ssh/sshd_config</filename>:
       </para>
      </listitem>
     </itemizedlist>
     <programlisting>
PubkeyAuthentication yes
     </programlisting>
     <itemizedlist>
      <listitem>
       <para>
        If SELinux is enabled, SSH public key authentication (passwordless SSH) may fail.
        You need to run the following command on all servers.
       </para>
      </listitem>
     </itemizedlist>
    <programlisting>
[all servers]# su - postgres
[all servers]$ restorecon -Rv ~/.ssh
    </programlisting>
    </para>
   </note>
  </sect3>

  <sect3 id="example-cluster-before-starting-pgpass">
   <title>Creating .pgpass</title>
   <para>
    To allow <literal>repl</literal> user without specifying password for streaming 
    replication and online recovery, and execute <application>pg_rewind</application>
    using <literal>postgres</literal>, we
    Create the <filename>.pgpass</filename> file in <literal>postgres</literal>
    user's home directory and change the permission to <literal>600</literal>
    on each <productname>PostgreSQL</productname> server.
    This file allows <literal>repl</literal> user and <literal>postgres</literal>
    user without providing a password for streaming replication and failover.
   </para>
   <programlisting>
[all servers]# su - postgres
[all servers]$ vi /var/lib/pgsql/.pgpass
server1:5432:replication:repl:&lt;repl user password&gt;
server2:5432:replication:repl:&lt;repl user password&gt;
server3:5432:replication:repl:&lt;repl user password&gt;
server1:5432:postgres:postgres:&lt;postgres user password&gt;
server2:5432:postgres:postgres:&lt;postgres user password&gt;
server3:5432:postgres:postgres:&lt;postgres user password&gt;
[all servers]$ chmod 600 /var/lib/pgsql/.pgpass
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-before-starting-firewall">
   <title>Setting up firewall</title>
   <para>
    When connect to <productname>Pgpool-II</productname> and <productname>PostgreSQL</productname> servers, the target port must be accessible by enabling firewall management softwares. Following is an example for <systemitem>Rocky Linux 8/RHEL 8</systemitem>.
   </para>
   <programlisting>
[all servers]# firewall-cmd --permanent --zone=public --add-service=postgresql
[all servers]# firewall-cmd --permanent --zone=public --add-port=9999/tcp --add-port=9898/tcp --add-port=9000/tcp  --add-port=9694/udp
[all servers]# firewall-cmd --reload
   </programlisting>
  </sect3>
 </sect2>

 <sect2 id="example-cluster-pgpool-node-id">
  <title>Create pgpool_node_id</title>
  <para>
    From <productname>Pgpool-II</productname> 4.2, now all configuration parameters are identical on all hosts.
    If <literal>watchdog</literal> feature is enabled, to distinguish which host is which,
    a <filename>pgpool_node_id</filename> file is required.
    You need to create a <filename>pgpool_node_id</filename> file and specify the pgpool (watchdog) node number
    (e.g. 0, 1, 2 ...) to identify pgpool (watchdog) host.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <literal>server1</literal>
    </para>
    <programlisting>
[server1]# cat /etc/pgpool-II/pgpool_node_id
0
    </programlisting>
   </listitem>
   <listitem>
    <para>
     <literal>server2</literal>
    </para>
    <programlisting>
[server2]# cat /etc/pgpool-II/pgpool_node_id
1
    </programlisting>
   </listitem>
   <listitem>
    <para>
     <literal>server3</literal>
    </para>
    <programlisting>
[server3]# cat /etc/pgpool-II/pgpool_node_id
2
    </programlisting>
   </listitem>
  </itemizedlist>
 </sect2>

 <sect2 id="pcp-authentication">
  <title>PCP connection authentication</title>
  <para>
   To use PCP commands PCP user names and md5 encrypted passwords must be
   declared in <filename>pcp.conf</filename> in format
   "<literal>username:encrypted password</literal>".
  </para>
  <para>
   In this example, we set PCP username to "<literal>pgpool</literal>"
   and password to "<literal>pgpool_password</literal>".
   Use <xref linkend="PG-MD5"> to create the encrypted password entry for
   <literal>pgpool</literal> user as below:
  </para>
  <programlisting>
[all servers]# echo 'pgpool:'`pg_md5 pgpool_password` &gt;&gt; /etc/pgpool-II/pcp.conf

[all servers]# cat /etc/pgpool-II/pcp.conf
# USERID:MD5PASSWD
pgpool:4aa0cb9673e84b06d4c8a848c80eb5d0
  </programlisting>
 </sect2>

 <sect2 id="example-cluster-pgpool-config">
  <title><productname>Pgpool-II</productname> Configuration</title>
  <para>
   When installing <productname>Pgpool-II</productname> using YUM, the
   <productname>Pgpool-II</productname> configuration file <filename>pgpool.conf</filename>
   is installed in <filename>/etc/pgpool-II</filename>.
  </para>
  <para>
   Since from <productname>Pgpool-II</productname> 4.2, all configuration parameters are
   identical on all hosts, you can edit <filename>pgpool.conf</filename> on any pgpool node
   and copy the edited <filename>pgpool.conf</filename> file to the other pgpool nodes.
  </para>

  <sect3 id="example-cluster-pgpool-config-config-file">
   <title>Clustering mode</title>
   <para>
    <productname>Pgpool-II</productname> has several clustering modes. To set the clustering
    mode, <xref linkend="GUC-BACKEND-CLUSTERING-MODE"> can be used. In this configuration
    example, streaming replication mode is used.
   </para>
   <programlisting>
backend_clustering_mode = 'streaming_replication'
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-listen-addresses">
   <title>listen_addresses</title>
   <para>
    To allow Pgpool-II and PCP to accept all incoming connections, set the following
    parameters to <literal>'*'</literal>.
   </para>
   <programlisting>
listen_addresses = '*'
pcp_listen_addresses = '*'
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-port">
   <title>port</title>
   <para>
    Specify the port number Pgpool-II listen on.
   </para>
   <programlisting>
port = 9999
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-sr-check">
   <title>Streaming Replication Check</title>
   <para>
    Specify replication delay check user and password in <xref linkend="GUC-SR-CHECK-USER">
    and <xref linkend="GUC-SR-CHECK-PASSWORD">. In this example, we leave
    <xref linkend="GUC-SR-CHECK-PASSWORD"> empty, and create the entry in
    <xref linkend="GUC-POOL-PASSWD">. See <xref linkend="example-cluster-pgpool-config-auth">
    for how to create the entry in <xref linkend="GUC-POOL-PASSWD">.
    From <productname>Pgpool-II</productname> 4.0, if these parameters are left blank,
    <productname>Pgpool-II</productname> will first try to get the password for that
    specific user from <xref linkend="GUC-POOL-PASSWD"> file before using the empty password.
   </para>
   <programlisting>
sr_check_user = 'pgpool'
sr_check_password = ''
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-health-check">
   <title>Health Check</title>
   <para>
    Enable health check so that <productname>Pgpool-II</> performs failover. Also, if the network is unstable,
    the health check fails even though the backend is running properly, failover or degenerate operation may occur.
    In order to prevent such incorrect detection of health check, we set <varname>health_check_max_retries = 3</varname>.
    Specify <xref linkend="GUC-HEALTH-CHECK-USER"> and <xref linkend="GUC-HEALTH-CHECK-PASSWORD"> in
      the same way like <xref linkend="GUC-SR-CHECK-USER"> and <xref linkend="GUC-SR-CHECK-PASSWORD">.
   </para>
   <programlisting>
health_check_period = 5
health_check_timeout = 30
health_check_user = 'pgpool'
health_check_password = ''
health_check_max_retries = 3
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-backend-settings">
   <title>Backend Settings</title>
   <para>
    Specify the <productname>PostgreSQL</productname> backend information.
    Multiple backends can be specified by adding a number at the end of the parameter name.
   </para>
   <programlisting>
# - Backend Connection Settings -

backend_hostname0 = 'server1'
backend_port0 = 5432
backend_weight0 = 1
backend_data_directory0 = '/var/lib/pgsql/16/data'
backend_flag0 = 'ALLOW_TO_FAILOVER'

backend_hostname1 = 'server2'
backend_port1 = 5432
backend_weight1 = 1
backend_data_directory1 = '/var/lib/pgsql/16/data'
backend_flag1 = 'ALLOW_TO_FAILOVER'

backend_hostname2 = 'server3'
backend_port2 = 5432
backend_weight2 = 1
backend_data_directory2 = '/var/lib/pgsql/16/data'
backend_flag2 = 'ALLOW_TO_FAILOVER'
   </programlisting>
   <para>
    To show <literal>replication_state</literal> and
    <literal>replication_sync_state</literal> column in
    <xref linkend="SQL-SHOW-POOL-NODES"> command result,
    <xref linkend="GUC-BACKEND-APPLICATION-NAME"> parameter is required.
    Here we specify each backend's hostname in these parameters
    (<productname>Pgpool-II</productname> 4.1 or later).
    Make sure that the value set in <varname>backend_application_nameX</varname>
    matches the value set in <varname>application_name</varname>
    of <varname>primary_conninfo</varname>.
   </para>
   <programlisting>
...
backend_application_name0 = 'server1'
...
backend_application_name1 = 'server2'
...
backend_application_name2 = 'server3'
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-failover">
   <title>Failover configuration</title>
   <para>
    Specify the script that will be executed when failover occurs in
    <xref linkend="GUC-FAILOVER-COMMAND">. When using three or more
    PostgreSQL servers, it's required to specify
    <xref linkend="GUC-FOLLOW-PRIMARY-COMMAND"> to synchronize the standby
    with the new primary. In case of two PostgreSQL servers, the setting of
    <xref linkend="GUC-FOLLOW-PRIMARY-COMMAND"> is not required.
   </para>
   <note>
    <para>
     When performing a switchover using <xref linkend="PCP-PROMOTE-NODE">
     with <option>switchover</option> option added in <productname>Pgpool-II</productname> 4.3,
     if you want to turn the former primary into standby automatically,
     <xref linkend="GUC-FOLLOW-PRIMARY-COMMAND"> is required to be set
     even if there are two PostgreSQL servers.
    </para>
   </note>
   <para>
    <productname>Pgpool-II</productname> replaces the following special characters with the backend specific
    information while executing the scripts. 
    See <xref linkend="GUC-FAILOVER-COMMAND"> and <xref linkend="GUC-FOLLOW-PRIMARY-COMMAND"> for more details about each character.
   </para>
   <programlisting>
failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R %N %S'
follow_primary_command = '/etc/pgpool-II/follow_primary.sh %d %h %p %D %m %H %M %P %r %R'
   </programlisting>
   <note>
    <para>
     <emphasis>%N</emphasis> and <emphasis>%S</emphasis> are added in <productname>Pgpool-II</productname> 4.1.
     Please note that these characters cannot be specified if using Pgpool-II 4.0 or earlier.
    </para>
   </note>
   <para>
    Sample scripts <ulink url="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/failover.sh.sample;hb=refs/heads/V4_5_STABLE">failover.sh</ulink>
    and <ulink url="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/follow_primary.sh.sample;hb=refs/heads/V4_5_STABLE">follow_primary.sh</ulink>
    are installed in <filename>/etc/pgpool-II/</filename>. Create failover scripts using these sample files.
   </para>
   <programlisting>
[all servers]# cp -p /etc/pgpool-II/sample_scripts/failover.sh.sample /etc/pgpool-II/failover.sh
[all servers]# cp -p /etc/pgpool-II/sample_scripts/follow_primary.sh.sample /etc/pgpool-II/follow_primary.sh
[all servers]# chown postgres:postgres /etc/pgpool-II/{failover.sh,follow_primary.sh}
   </programlisting>
   <para>
    Basically, it should work if you change <emphasis>PGHOME</emphasis> according to
    PostgreSQL installation directory.
   </para>
   <programlisting>
[all servers]# vi /etc/pgpool-II/failover.sh
...
PGHOME=/usr/pgsql-16
...

[all servers]# vi /etc/pgpool-II/follow_primary.sh
...
PGHOME=/usr/pgsql-16
...
   </programlisting>

   <para>
    Make sure the entry of the PCP user specified in <varname>PCP_USER</varname> in
    <filename>follow_primary.sh</filename> is created in <filename>pcp.conf</filename>.
    In this example, we have created in <xref linkend="PCP-AUTHENTICATION">
   </para>
   <programlisting>
# cat /etc/pgpool-II/follow_primary.sh
...
PCP_USER=pgpool
...
   </programlisting>
   <para>
    Since <filename>follow_primary.sh</filename> script must execute PCP command without
    entering a password, we need to create <filename>.pcppass</filename> in
    <literal>postgres</literal> user's home directory on each server
    (the home directory of the user Pgpool-II is running as).
    The format of <filename>.pcppass</filename> is
    "<literal>hostname:port:username:password</literal>".
   </para>
   <para>
    In this example, we assume that the PCP user is <literal>pgpool</literal>
    and the password is <literal>pgpool_password</literal>.
   </para>
   <programlisting>
[all servers]# su - postgres
[all servers]$ echo 'localhost:9898:pgpool:pgpool_password' &gt; ~/.pcppass
[all servers]$ chmod 600 ~/.pcppass
   </programlisting>
   <note>
    <para>
     The <filename>follow_primary.sh</filename> script does not support tablespaces.
     If you are using tablespaces, you need to modify the script to support tablespaces.
    </para>
   </note>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-online-recovery">
   <title>Pgpool-II Online Recovery Configurations</title>
   <para>
    Next, configure the required parameters to perform online recovery.
    Because <emphasis>Superuser</emphasis> privilege in <productname>PostgreSQL</productname>
    is required for performing online recovery, we specify <literal>postgres</literal>
    user in <xref linkend="GUC-RECOVERY-USER">. In this example, we leave
    <xref linkend="GUC-RECOVERY-PASSWORD"> empty, and create the entry in
    <xref linkend="GUC-POOL-PASSWD">. See <xref linkend="example-cluster-pgpool-config-auth">
    for how to create the entry in <xref linkend="GUC-POOL-PASSWD">.
   </para>
   <programlisting>
recovery_user = 'postgres'
recovery_password = ''
recovery_1st_stage_command = 'recovery_1st_stage'
   </programlisting>
   <para>
    Then, we create <filename>recovery_1st_stage</filename> and <filename>pgpool_remote_start</filename>
    in database cluster directory of <productname>PostgreSQL</productname>
    primary server (server1).
   </para>
   <para>
    The sample scripts of online recovery <ulink url="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/recovery_1st_stage.sample;hb=refs/heads/V4_5_STABLE">recovery_1st_stage</ulink>
    and <ulink url="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/pgpool_remote_start.sample;hb=refs/heads/V4_5_STABLE">pgpool_remote_start</ulink>
    are installed in <filename>/etc/pgpool-II/</filename>. Copy these files to the data directory of the primary server (server1).
   </para>
   <programlisting>
[server1]# cp -p /etc/pgpool-II/sample_scripts/recovery_1st_stage.sample /var/lib/pgsql/16/data/recovery_1st_stage
[server1]# cp -p /etc/pgpool-II/sample_scripts/pgpool_remote_start.sample /var/lib/pgsql/16/data/pgpool_remote_start
[server1]# chown postgres:postgres /var/lib/pgsql/16/data/{recovery_1st_stage,pgpool_remote_start}
   </programlisting>
   <para>
    Basically, it should work if you change <emphasis>PGHOME</emphasis> according to PostgreSQL installation directory.
   </para>
   <programlisting>
[server1]# vi /var/lib/pgsql/16/data/recovery_1st_stage
...
PGHOME=/usr/pgsql-16
...

[server1]# vi /var/lib/pgsql/16/data/pgpool_remote_start
...
PGHOME=/usr/pgsql-16
...
   </programlisting>

   <para>
    In order to use the online recovery functionality, the functions of
    <function>pgpool_recovery</function>, <function>pgpool_remote_start</function>,
    <function>pgpool_switch_xlog</function> are required, so we need to install
    <function>pgpool_recovery</function> on template1 of <productname>PostgreSQL</productname> server
    <literal>server1</literal>.
   </para>
   <programlisting>
[server1]# su - postgres
[server1]$ psql template1 -c "CREATE EXTENSION pgpool_recovery"
   </programlisting>
   <note>
    <para>
     The <filename>recovery_1st_stage</filename> script does not support tablespaces.
     If you are using tablespaces, you need to modify the script to support tablespaces.
    </para>
   </note>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-auth">
   <title>Client Authentication Configuration</title>
   <para>
    Because in the section <link linkend="EXAMPLE-CLUSTER-PRE-SETUP">Before Starting</link>,
    we already set <productname>PostgreSQL</productname> authentication method to
    <acronym>scram-sha-256</acronym>, it is necessary to set a client authentication by
    <productname>Pgpool-II</productname> to connect to backend nodes.
    When installing with RPM, the <productname>Pgpool-II</productname> configuration file
    <filename>pool_hba.conf</filename> is in <filename>/etc/pgpool-II</filename>.
    By default, pool_hba authentication is disabled, set <varname>enable_pool_hba = on</varname>
    to enable it.
   </para>
   <programlisting>
enable_pool_hba = on
   </programlisting>
   <para>
    The format of <filename>pool_hba.conf</filename> file follows
    PostgreSQL's <filename>pg_hba.conf</filename> format very closely.
    Set <literal>pgpool</literal> and <literal>postgres</literal> user's
    authentication method to <literal>scram-sha-256</literal>. In this example,
    it is assumed that the application connecting to <productname>Pgpool-II</productname>
    is in the same subnet.
   </para>
   <programlisting>
host    all         pgpool           samenet          scram-sha-256
host    all         postgres         samenet          scram-sha-256
   </programlisting>
   <note>
    <para>
     Please note that in <productname>Pgpool-II</productname> 4.0 only AES encrypted password or clear text password
     can be specified in <xref linkend="guc-health-check-password">, <xref linkend="guc-sr-check-password">, 
       <xref linkend="guc-wd-lifecheck-password">, <xref linkend="guc-recovery-password"> in <filename>pgpool.conf</filename>.
    </para>
   </note>
   <para>
    The default password file name for authentication is <xref linkend="GUC-POOL-PASSWD">.
     To use <literal>scram-sha-256</literal> authentication, the decryption key to
     decrypt the passwords is required. We create the <literal>.pgpoolkey</literal>
     file in <literal>postgres</literal> user's home directory
     (the user Pgpool-II is running as. <productname>Pgpool-II</productname> 4.0 or before,
     <productname>Pgpool-II</productname> is running as <literal>root</literal> by default)
     <programlisting>
[all servers]# su - postgres
[all servers]$ echo 'some string' > ~/.pgpoolkey
[all servers]$ chmod 600 ~/.pgpoolkey
     </programlisting>
   </para>
   <para>
    Execute command <command>pg_enc -m -k /path/to/.pgpoolkey -u username -p</command> to register user
    name and <literal>AES</literal> encrypted password in file <filename>pool_passwd</filename>.
    If <filename>pool_passwd</filename> doesn't exist yet, it will be created in the same directory as
    <filename>pgpool.conf</filename>.
   </para>
   <programlisting>
[all servers]# su - postgres
[all servers]$ pg_enc -m -k ~/.pgpoolkey -u pgpool -p
db password: [pgpool user's password]
[all servers]$ pg_enc -m -k ~/.pgpoolkey -u postgres -p
db password: [postgres user's password]

# cat /etc/pgpool-II/pool_passwd
pgpool:AESheq2ZMZjynddMWk5sKP/Rw==
postgres:AESHs/pWL5rtXy2IwuzroHfqg==
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-watchdog">
   <title>Watchdog Configuration</title>
   <para>
    Enable watchdog functionality on <literal>server1</literal>, <literal>server2</literal>, <literal>server3</literal>.
   </para>
   <programlisting>
use_watchdog = on
   </programlisting>
   <para>
    Set virtual IP address to <xref linkend="GUC-DELEGATE-IP">.
    Ensure that the IP address isn't used yet.
   </para>
   <programlisting>
delegate_ip = '192.168.100.50'
   </programlisting>

   <para>
    To bring up/down the virtual IP and send the ARP requests, we set <xref linkend="GUC-IF-UP-CMD">, <xref linkend="GUC-IF-DOWN-CMD"> and <xref linkend="GUC-ARPING-CMD">.
    The network interface used in this example is "enp0s8".
    Since root privilege is required to execute <varname>if_up/down_cmd</varname> or
    <varname>arping_cmd</varname> command, use setuid on these command or allow
    <literal>postgres</literal> user (the user Pgpool-II is running as) to run
    <command>sudo</command> command without a password.
   </para>
   <note>
    <para>
    If <productname>Pgpool-II</productname> is installed using RPM, the <literal>postgres</literal>
    user has been configured to run <command>ip/arping</command> via <command>sudo</command> without
    a password.
    <programlisting>
postgres ALL=NOPASSWD: /sbin/ip
postgres ALL=NOPASSWD: /usr/sbin/arping
    </programlisting>
    </para>
   </note>
   <para>
    Here we configure the following parameters to run <varname>if_up/down_cmd</varname> or <varname>arping_cmd</varname> with sudo.
   </para>
   <programlisting>
if_up_cmd = '/usr/bin/sudo /sbin/ip addr add $_IP_$/24 dev enp0s8 label enp0s8:0'
if_down_cmd = '/usr/bin/sudo /sbin/ip addr del $_IP_$/24 dev enp0s8'
arping_cmd = '/usr/bin/sudo /usr/sbin/arping -U $_IP_$ -w 1 -I enp0s8'
   </programlisting>
   <note>
    <para>
     If "Defaults requiretty" is set in the <filename>/etc/sudoers</filename>,
     please ensure that the user that <productname>Pgpool-II</productname> is running as can execute the <command>if_up_cmd</command>, <command>if_down_cmd</command> and <command>arping_cmd</command> command without a tty.
    </para>
   </note>
   <para>
    Set <xref linkend="GUC-IF-CMD-PATH"> and <xref linkend="GUC-ARPING-PATH"> according to the
    command path.
    If <varname>if_up/down_cmd</varname> or <varname>arping_cmd</varname> starts with "/", these parameters will be ignored. 
   </para>
   <programlisting>
if_cmd_path = '/sbin'
arping_path = '/usr/sbin'
   </programlisting>
   <para>
    Specify all <productname>Pgpool-II</productname> nodes information for configuring watchdog.
    Specify <varname>pgpool_portX</varname> using the port number specified in <varname>port</varname> in
    <xref linkend="example-cluster-pgpool-config-port">.
   </para>
   <programlisting>
hostname0 = 'server1'
wd_port0 = 9000
pgpool_port0 = 9999

hostname1 = 'server2'
wd_port1 = 9000
pgpool_port1 = 9999

hostname2 = 'server3'
wd_port2 = 9000
pgpool_port2 = 9999
   </programlisting>
   <para>
    Configure the method of lifecheck <xref linkend="guc-wd-lifecheck-method">
    and the lifecheck interval <xref linkend="guc-wd-interval">.
    Here, we use <literal>heartbeat</literal> method to perform watchdog lifecheck.
   </para>
   <programlisting>
wd_lifecheck_method = 'heartbeat'
wd_interval = 10
   </programlisting>
   <para>
    Specify all <productname>Pgpool-II</productname> nodes information for sending and receiving heartbeat signal.
   </para>
   <programlisting>
heartbeat_hostname0 = 'server1'
heartbeat_port0 = 9694
heartbeat_device0 = ''
heartbeat_hostname1 = 'server2'
heartbeat_port1 = 9694
heartbeat_device1 = ''
heartbeat_hostname2 = 'server3'
heartbeat_port2 = 9694
heartbeat_device2 = ''
   </programlisting>
   <para>
    If <xref linkend="guc-wd-lifecheck-method"> is set to <literal>heartbeat</literal>,
    specify the time to detect a fault <xref linkend="guc-wd-heartbeat-deadtime"> and
    the interval to send heartbeat signals <xref linkend="guc-wd-heartbeat-deadtime">.
   </para>
   <programlisting>
wd_heartbeat_keepalive = 2
wd_heartbeat_deadtime = 30
   </programlisting>

   <para>
     This setting is optional.
     When <literal>Watchdog</literal> process is abnormally terminated,
     the virtual IP may be "up" on both of the old and new active pgpool nodes.
     To prevent this, configure <xref linkend="guc-wd-escalation-command">
     to bring down the virtual IP on other Pgpool-II nodes before
     bringing up the virtual IP on the new leader Pgpool-II node.
   </para>
    <programlisting>
wd_escalation_command = '/etc/pgpool-II/escalation.sh'
    </programlisting>
   <para>
    The sample script <ulink url="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/escalation.sh.sample;hb=refs/heads/V4_5_STABLE">escalation.sh</ulink> is installed in <filename>/etc/pgpool-II/</filename>.
   </para>
    <programlisting>
[all servers]# cp -p /etc/pgpool-II/sample_scripts/escalation.sh.sample /etc/pgpool-II/escalation.sh
[all servers]# chown postgres:postgres /etc/pgpool-II/escalation.sh
    </programlisting>

   <para>
    Basically, it should work if you change the following variables according to your environment.
    PGPOOLS is a list of hostnames where Pgpool-II is running.
    VIP is the virtual IP address that is set to <xref linkend="guc-delegate-ip">.
    DEVICE is the network interface for the virtual IP.
   </para>
    <programlisting>
[all servers]# vi /etc/pgpool-II/escalation.sh
...
PGPOOLS=(server1 server2 server3)
VIP=192.168.100.50
DEVICE=enp0s8
...
    </programlisting>

   <note>
    <para>
     If you have even number of watchdog nodes, you need to turn on <xref linkend="guc-enable-consensus-with-half-votes"> parameter.
    </para>
   </note>
   <note>
    <para>
     If <varname>use_watchdog = on</varname>, please make sure the pgpool
     node number is specified in <filename>pgpool_node_id</filename> file.
     See <xref linkend="example-cluster-pgpool-node-id"> for details.
    </para>
   </note>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-log">
   <title>Logging</title>
   <para>
    Since Pgpool-II 4.2, the logging collector process has been implemented.
    In the example, we enable logging collector.
   </para>
   <programlisting>
log_destination = 'stderr'
logging_collector = on
log_directory = '/var/log/pgpool_log'
log_filename = 'pgpool-%a.log'
log_truncate_on_rotation = on
log_rotation_age = 1d
log_rotation_size = 10MB
   </programlisting>
   <para>
    Create the log directory on all servers.
   </para>
   <programlisting>
[all servers]# mkdir /var/log/pgpool_log/
[all servers]# chown postgres:postgres /var/log/pgpool_log/
   </programlisting>

  <para>
   The configuration of <filename>pgpool.conf</filename> on server1 is completed.
   Copy the <filename>pgpool.conf</filename> to other
   <productname>Pgpool-II</productname> nodes (server2 and server3).
  </para>
  <programlisting>
[server1]# scp -p /etc/pgpool-II/pgpool.conf root@server2:/etc/pgpool-II/pgpool.conf
[server1]# scp -p /etc/pgpool-II/pgpool.conf root@server3:/etc/pgpool-II/pgpool.conf
  </programlisting>
  </sect3>
 </sect2>

 <sect2 id="example-cluster-verify">
  <title>How to use</title>
  <para>
   Let's start to use <productname>Pgpool-II</productname>.
  </para>
  <sect3 id="example-cluster-verify-starting-stopping">
   <title>Starting/Stopping Pgpool-II</title>
   <itemizedlist>
    <listitem>
     <para>
      Starting <productname>Pgpool-II</productname>
     </para>
     <para>
      First, let's start <productname>Pgpool-II</productname>.
     </para>
     <para>
      Before starting <productname>Pgpool-II</productname>,
      the PostgreSQL primary server must be already running.
      If PostgreSQL primary server is not running, start it first
      using the following command.
     </para>
     <programlisting>
[server1]# su - postgres
[server1]$ /usr/pgsql-16/bin/pg_ctl start -D $PGDATA
     </programlisting>
     <para>
      Start <productname>Pgpool-II</productname> on <literal>server1</literal>,
      <literal>server2</literal>, <literal>server3</literal> by using the following command.
     </para>
     <programlisting>
[all servers]# systemctl start pgpool.service
     </programlisting>
    </listitem>
    <listitem>
     <para>
      Stopping <productname>Pgpool-II</productname>
     </para>
     <para>
      When stopping <productname>PostgreSQL</productname>,
      <productname>Pgpool-II</productname> must be stopped first.
     </para>
     <programlisting>
[all servers]# systemctl stop pgpool.service
     </programlisting>
    </listitem>
   </itemizedlist>
  </sect3>

  <sect3 id="example-cluster-verify-standby">
   <title>Setting up PostgreSQL standby server</title>
   <para>
    First, we should set up <productname>PostgreSQL</productname> standby server by
    using <productname>Pgpool-II</productname> online recovery functionality.
   </para>
   <para>
    Connect to Pgpool-II via virtual IP to check the status of backend nodes.
    As shown in the result, primary server is running on <literal>server1</literal>,
    standby servers on <literal>server2</literal> and <literal>server3</literal>
    are in "down" status.
   </para>
   <programlisting>
[any server]# psql -h 192.168.100.50 -p 9999 -U pgpool postgres -c "show pool_nodes"
Password for user pgpool: 
 node_id | hostname | port | status | pg_status | lb_weight |  role   | pg_role | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
---------+----------+------+--------+-----------+-----------+---------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
 0       | server1  | 5432 | up     | up        | 0.333333  | primary | primary | 0          | true              | 0                 |                   |                        | 2023-11-10 15:30:14
 1       | server2  | 5432 | down   | down      | 0.333333  | standby | unknown | 0          | false             | 0                 |                   |                        | 2023-11-10 15:30:14
 2       | server3  | 5432 | down   | down      | 0.333333  | standby | unknown | 0          | false             | 0                 |                   |                        | 2023-11-10 15:30:14
(3 rows)
   </programlisting>
   <para>
    Before running <xref linkend="pcp-recovery-node"> command,
    ensure that <filename>recovery_1st_stage</filename> and
    <filename>pgpool_remote_start</filename> scripts exist in the
    data directory of <productname>PostgreSQL</productname>
    primary server (<literal>server1</literal>).
   </para>
   <programlisting>
[any server]# pcp_recovery_node -h 192.168.100.50 -p 9898 -U pgpool -n 1 -W
Password:
pcp_recovery_node -- Command Successful

[any server]# pcp_recovery_node -h 192.168.100.50 -p 9898 -U pgpool -n 2 -W
Password:
pcp_recovery_node -- Command Successful
   </programlisting>
   <para>
    After executing <command>pcp_recovery_node</command> command,
    verify that <productname>PostgreSQL</productname> standby servers
    are running on <literal>server2</literal> and <literal>server3</literal>.
   </para>
   <programlisting>
[any server]# psql -h 192.168.100.50 -p 9999 -U pgpool postgres -c "show pool_nodes"
Password for user pgpool
 node_id | hostname | port | status | pg_status | lb_weight |  role   | pg_role | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change
---------+----------+------+--------+-----------+-----------+---------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
 0       | server1  | 5432 | up     | up        | 0.333333  | primary | primary | 0          | false             | 0                 |                   |                        | 2023-11-10 15:30:14
 1       | server2  | 5432 | up     | up        | 0.333333  | standby | standby | 0          | true              | 0                 | streaming         | async                  | 2023-11-10 16:32:33
 2       | server3  | 5432 | up     | up        | 0.333333  | standby | standby | 0          | false             | 0                 | streaming         | async                  | 2023-11-10 16:33:08
(3 rows)
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-verify-watchdog">
   <title>Switching leader/standby watchdog</title>
   <para>
    Confirm the watchdog status by using <command>pcp_watchdog_info</command>. The <command>Pgpool-II</command> server which is started first runs as <literal>LEADER</literal>.
   </para>
   <programlisting>
[any server]# pcp_watchdog_info -h 192.168.100.50 -p 9898 -U pgpool -W
Password:
3 3 YES server1:9999 Linux server1 server1

server1:9999 Linux server1 server1 9999 9000 4 LEADER 0 MEMBER  # The Pgpool-II server started first becomes "LEADER".
server2:9999 Linux server2 server2 9999 9000 7 STANDBY 0 MEMBER # running as STANDBY
server3:9999 Linux server3 server3 9999 9000 7 STANDBY 0 MEMBER # running as STANDBY
   </programlisting>
   <para>
    If the <literal>LEADER</literal> <productname>Pgpool-II</productname>
    on <literal>server1</literal> goes down, standby
    <productname>Pgpool-II</productname> on <literal>server2</literal> or 
    <literal>server3</literal> will become the new <literal>LEADER</literal>.
   </para>
   <para>
    To verify this behavior, you may stop <productname>Pgpool-II</productname> 
    service or shutdown the whole system. Here, we stop
    <productname>Pgpool-II</productname> service.
   </para>
   <programlisting>
[server1]# systemctl stop pgpool.service

[server1]# pcp_watchdog_info -p 9898 -h 192.168.100.50 -U pgpool -W
Password:
3 3 YES server2:9999 Linux server2 server2

server2:9999 Linux server2 server2 9999 9000 4 LEADER 0 MEMBER    # server2 becomes LEADER
server1:9999 Linux server1 server1 9999 9000 10 SHUTDOWN 0 MEMBER # server1 is stopped
server3:9999 Linux server3 server3 9999 9000 7 STANDBY 0 MEMBER   # server3 is running as a STANDBY
   </programlisting>
   <para>
    Restart the stopped <productname>Pgpool-II</productname>
    on <literal>server1</literal> and verify that it is running
    as a <literal>STANDBY</literal>.
   </para>
   <programlisting>
[server1]# systemctl start pgpool.service

[server1]# pcp_watchdog_info -p 9898 -h 192.168.100.50 -U pgpool -W
Password: 
3 3 YES server2:9999 Linux server2 server2

server2:9999 Linux server2 server2 9999 9000 4 LEADER 0 MEMBER
server1:9999 Linux server1 server1 9999 9000 7 STANDBY 0 MEMBER
server3:9999 Linux server3 server3 9999 9000 7 STANDBY 0 MEMBER
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-verify-failover">
   <title>Failover</title>
   <para>
    First, use <command>psql</command> to connect to
    <productname>PostgreSQL</productname> via virtual IP,
    and verify the backend information.
   </para>
   <programlisting>
# psql -h 192.168.100.50 -p 9999 -U pgpool postgres -c "show pool_nodes"
Password for user pgpool:
 node_id | hostname | port | status | pg_status | lb_weight |  role   | pg_role | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change
---------+----------+------+--------+-----------+-----------+---------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
 0       | server1  | 5432 | up     | up        | 0.333333  | primary | primary | 0          | false             | 0                 |                   |                        | 2023-11-10 15:30:14
 1       | server2  | 5432 | up     | up        | 0.333333  | standby | standby | 0          | false             | 0                 | streaming         | async                  | 2023-11-10 16:32:33
 2       | server3  | 5432 | up     | up        | 0.333333  | standby | standby | 0          | true              | 0                 | streaming         | async                  | 2023-11-10 16:33:08
(3 rows)
   </programlisting>
   <para>
    Next, stop the primary <productname>PostgreSQL</productname> server 
    on <literal>server1</literal> and verify that failover is performed
    automatically.
   </para>
   <programlisting>
[server1]$ pg_ctl -D /var/lib/pgsql/16/data -m immediate stop
   </programlisting>
   <para>
    After stopping <productname>PostgreSQL</productname> on
    <literal>server1</literal>, failover occurs.
    <productname>PostgreSQL</productname> on 
    <literal>server2</literal> becomes the new primary and
    the standby server on <literal>server3</literal>
    is configured as a standby of the new primary.
   </para>
   <programlisting>
[any server]# psql -h 192.168.100.50 -p 9999 -U pgpool postgres -c "show pool_nodes"
Password for user pgpool:
 node_id | hostname | port | status | pg_status | lb_weight |  role   | pg_role | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change
---------+----------+------+--------+-----------+-----------+---------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
 0       | server1  | 5432 | down   | down      | 0.333333  | standby | unknown | 0          | false             | 0                 |                   |                        | 2023-11-10 17:05:40
 1       | server2  | 5432 | up     | up        | 0.333333  | primary | primary | 0          | false             | 0                 |                   |                        | 2023-11-10 17:05:40
 2       | server3  | 5432 | up     | up        | 0.333333  | standby | standby | 0          | true              | 0                 | streaming         | async                  | 2023-11-10 17:05:51
(3 rows)
   </programlisting>
   <para>
    <literal>server3</literal> is running as a standby of new primary <literal>server2</literal>.
   </para>

   <programlisting>
[server3]# psql -h server3 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
pg_is_in_recovery 
-------------------
t

[server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
pg_is_in_recovery 
-------------------
f

[server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select * from pg_stat_replication" -x
-[ RECORD 1 ]----+------------------------------
pid              | 7198
usesysid         | 16385
usename          | repl
application_name | server3
client_addr      | 192.168.100.53
client_hostname  |
client_port      | 40916
backend_start    | 2023-11-10 17:10:03.067241+00
backend_xmin     |
state            | streaming
sent_lsn         | 0/12000260
write_lsn        | 0/12000260
flush_lsn        | 0/12000260
replay_lsn       | 0/12000260
write_lag        |
flush_lag        |
replay_lag       |
sync_priority    | 0
sync_state       | async
reply_time       | 2023-11-10 17:17:23.886477+00
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-verify-online-recovery">
   <title>Online Recovery</title>
   <para>
    Here, we use <productname>Pgpool-II</productname> online recovery
    feature to restore the former primary on <literal>server1</literal>
    as a standby.
   </para>
   <programlisting>
[any server]# pcp_recovery_node -h 192.168.100.50 -p 9898 -U pgpool -n 0 -W
Password: 
pcp_recovery_node -- Command Successful
   </programlisting>
   <para>
    Then verify that PostgreSQL on <literal>server1</literal> is
    running as a standby.
   </para>
   <programlisting>
[any server]# psql -h 192.168.100.50 -p 9999 -U pgpool postgres -c "show pool_nodes"
Password for user pgpool:
node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
---------+----------+------+--------+-----------+-----------+---------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
 0       | server1  | 5432 | up     | up        | 0.333333  | standby | standby | 0          | true              | 0                 | streaming         | async                  | 2023-11-10 17:22:03
 1       | server2  | 5432 | up     | up        | 0.333333  | primary | primary | 0          | false             | 0                 |                   |                        | 2023-11-10 17:05:40
 2       | server3  | 5432 | up     | up        | 0.333333  | standby | standby | 0          | false             | 0                 | streaming         | async                  | 2023-11-10 17:05:51
(3 rows)
   </programlisting>
  </sect3>
 </sect2>
</sect1>
